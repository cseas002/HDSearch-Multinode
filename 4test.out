Using /users/cseas002/HDSearch-Multinode/ansible.cfg as config file

PLAY [Install HDSearch] ********************************************************

TASK [Gathering Facts] *********************************************************
ok: [node1]
ok: [node2]

PLAY [Set Up Docker Curl] ******************************************************

TASK [Gathering Facts] *********************************************************
ok: [node1]
ok: [node2]
ok: [node0]

PLAY [Make space to commit] ****************************************************

TASK [Gathering Facts] *********************************************************
ok: [node2]
ok: [node1]
ok: [node0]

TASK [Make Space to Commit Image] **********************************************
[WARNING]: Consider using 'become', 'become_method', and 'become_user' rather
than running sudo
changed: [node1] => {"changed": true, "cmd": ["sudo", "/users/cseas002/HDSearch-Multinode/scripts/change-storage-location-docker.sh"], "delta": "0:00:03.601277", "end": "2023-10-10 12:51:44.148639", "rc": 0, "start": "2023-10-10 12:51:40.547362", "stderr": "\"docker rm\" requires at least 1 argument.\nSee 'docker rm --help'.\n\nUsage:  docker rm [OPTIONS] CONTAINER [CONTAINER...]\n\nRemove one or more containers\n\"docker rmi\" requires at least 1 argument.\nSee 'docker rmi --help'.\n\nUsage:  docker rmi [OPTIONS] IMAGE [IMAGE...]\n\nRemove one or more images\nWarning: Stopping docker.service, but it can still be activated by:\n  docker.socket\nmkdir: cannot create directory ‘/dev/mkdocker’: File exists", "stderr_lines": ["\"docker rm\" requires at least 1 argument.", "See 'docker rm --help'.", "", "Usage:  docker rm [OPTIONS] CONTAINER [CONTAINER...]", "", "Remove one or more containers", "\"docker rmi\" requires at least 1 argument.", "See 'docker rmi --help'.", "", "Usage:  docker rmi [OPTIONS] IMAGE [IMAGE...]", "", "Remove one or more images", "Warning: Stopping docker.service, but it can still be activated by:", "  docker.socket", "mkdir: cannot create directory ‘/dev/mkdocker’: File exists"], "stdout": "", "stdout_lines": []}
changed: [node0] => {"changed": true, "cmd": ["sudo", "/users/cseas002/HDSearch-Multinode/scripts/change-storage-location-docker.sh"], "delta": "0:00:04.281375", "end": "2023-10-10 12:51:44.847440", "rc": 0, "start": "2023-10-10 12:51:40.566065", "stderr": "\"docker rm\" requires at least 1 argument.\nSee 'docker rm --help'.\n\nUsage:  docker rm [OPTIONS] CONTAINER [CONTAINER...]\n\nRemove one or more containers\n\"docker rmi\" requires at least 1 argument.\nSee 'docker rmi --help'.\n\nUsage:  docker rmi [OPTIONS] IMAGE [IMAGE...]\n\nRemove one or more images\nWarning: Stopping docker.service, but it can still be activated by:\n  docker.socket\nmkdir: cannot create directory ‘/dev/mkdocker’: File exists", "stderr_lines": ["\"docker rm\" requires at least 1 argument.", "See 'docker rm --help'.", "", "Usage:  docker rm [OPTIONS] CONTAINER [CONTAINER...]", "", "Remove one or more containers", "\"docker rmi\" requires at least 1 argument.", "See 'docker rmi --help'.", "", "Usage:  docker rmi [OPTIONS] IMAGE [IMAGE...]", "", "Remove one or more images", "Warning: Stopping docker.service, but it can still be activated by:", "  docker.socket", "mkdir: cannot create directory ‘/dev/mkdocker’: File exists"], "stdout": "", "stdout_lines": []}
changed: [node2] => {"changed": true, "cmd": ["sudo", "/users/cseas002/HDSearch-Multinode/scripts/change-storage-location-docker.sh"], "delta": "0:00:04.608318", "end": "2023-10-10 12:51:45.154165", "rc": 0, "start": "2023-10-10 12:51:40.545847", "stderr": "\"docker rm\" requires at least 1 argument.\nSee 'docker rm --help'.\n\nUsage:  docker rm [OPTIONS] CONTAINER [CONTAINER...]\n\nRemove one or more containers\n\"docker rmi\" requires at least 1 argument.\nSee 'docker rmi --help'.\n\nUsage:  docker rmi [OPTIONS] IMAGE [IMAGE...]\n\nRemove one or more images\nWarning: Stopping docker.service, but it can still be activated by:\n  docker.socket\nmkdir: cannot create directory ‘/dev/mkdocker’: File exists", "stderr_lines": ["\"docker rm\" requires at least 1 argument.", "See 'docker rm --help'.", "", "Usage:  docker rm [OPTIONS] CONTAINER [CONTAINER...]", "", "Remove one or more containers", "\"docker rmi\" requires at least 1 argument.", "See 'docker rmi --help'.", "", "Usage:  docker rmi [OPTIONS] IMAGE [IMAGE...]", "", "Remove one or more images", "Warning: Stopping docker.service, but it can still be activated by:", "  docker.socket", "mkdir: cannot create directory ‘/dev/mkdocker’: File exists"], "stdout": "", "stdout_lines": []}

PLAY [Install Profiler Dep] ****************************************************

PLAY RECAP *********************************************************************
node0                      : ok=3    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
node1                      : ok=4    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
node2                      : ok=4    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

Using /users/cseas002/HDSearch-Multinode/ansible.cfg as config file
[WARNING]: While constructing a mapping from /users/cseas002/HDSearch-
Multinode/ansible/profiler.yml, line 20, column 3, found a duplicate dict key
(hosts). Using last defined value only.
[WARNING]: While constructing a mapping from /users/cseas002/HDSearch-
Multinode/ansible/profiler.yml, line 20, column 3, found a duplicate dict key
(tags). Using last defined value only.
[WARNING]: While constructing a mapping from /users/cseas002/HDSearch-
Multinode/ansible/profiler.yml, line 20, column 3, found a duplicate dict key
(tasks). Using last defined value only.

PLAY [Set profiler] ************************************************************

TASK [Gathering Facts] *********************************************************
ok: [node1]
ok: [node2]

TASK [Set profiler Hosts] ******************************************************
changed: [node2] => {"ansible_job_id": "599552152862.24067", "changed": true, "finished": 0, "results_file": "/users/cseas002/.ansible_async/599552152862.24067", "started": 1}
changed: [node1] => {"ansible_job_id": "586548159980.25893", "changed": true, "finished": 0, "results_file": "/users/cseas002/.ansible_async/586548159980.25893", "started": 1}

PLAY [Run remote profiler] *****************************************************

PLAY [Kill remote profiler] ****************************************************

PLAY RECAP *********************************************************************
node1                      : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
node2                      : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

Using /users/cseas002/HDSearch-Multinode/ansible.cfg as config file

PLAY [Initialize cluster manager] **********************************************

PLAY [Initialize workers] ******************************************************

PLAY [Check Status of run] *****************************************************

PLAY [Leave Swarm] *************************************************************

TASK [Gathering Facts] *********************************************************
ok: [node1]
ok: [node0]
ok: [node2]

TASK [Leave Swarm] *************************************************************
changed: [node2] => {"ansible_job_id": "502210665057.24204", "changed": true, "finished": 0, "results_file": "/users/cseas002/.ansible_async/502210665057.24204", "started": 1}
changed: [node0] => {"ansible_job_id": "216903827760.9646", "changed": true, "finished": 0, "results_file": "/users/cseas002/.ansible_async/216903827760.9646", "started": 1}
changed: [node1] => {"ansible_job_id": "197703902807.26030", "changed": true, "finished": 0, "results_file": "/users/cseas002/.ansible_async/197703902807.26030", "started": 1}

PLAY RECAP *********************************************************************
node0                      : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
node1                      : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
node2                      : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

Using /users/cseas002/HDSearch-Multinode/ansible.cfg as config file

PLAY [Initialize cluster manager] **********************************************

TASK [Gathering Facts] *********************************************************
ok: [node0]

TASK [Init Master] *************************************************************
[WARNING]: Consider using 'become', 'become_method', and 'become_user' rather
than running sudo
changed: [node0] => {"changed": true, "cmd": "sudo docker swarm init --advertise-addr 10.10.1.1", "delta": "0:00:00.597502", "end": "2023-10-10 12:51:54.878785", "rc": 0, "start": "2023-10-10 12:51:54.281283", "stderr": "", "stderr_lines": [], "stdout": "Swarm initialized: current node (tjuiyqfwq6qdmuio5x50f1hrw) is now a manager.\n\nTo add a worker to this swarm, run the following command:\n\n    docker swarm join --token SWMTKN-1-1ap320tvsvz49t6nqblgsq50117xqyx5v2fdfk65sb2wxed4xw-eubp7ys0g2jrvkv8x9vnc30zq 10.10.1.1:2377\n\nTo add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.", "stdout_lines": ["Swarm initialized: current node (tjuiyqfwq6qdmuio5x50f1hrw) is now a manager.", "", "To add a worker to this swarm, run the following command:", "", "    docker swarm join --token SWMTKN-1-1ap320tvsvz49t6nqblgsq50117xqyx5v2fdfk65sb2wxed4xw-eubp7ys0g2jrvkv8x9vnc30zq 10.10.1.1:2377", "", "To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions."]}

PLAY [Initialize workers] ******************************************************

PLAY [Check Status of run] *****************************************************

PLAY [Leave Swarm] *************************************************************

PLAY RECAP *********************************************************************
node0                      : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

Using /users/cseas002/HDSearch-Multinode/ansible.cfg as config file

PLAY [Initialize cluster manager] **********************************************

PLAY [Initialize workers] ******************************************************

TASK [Gathering Facts] *********************************************************
ok: [node1]
ok: [node2]

TASK [Init Workers] ************************************************************
changed: [node2] => {"ansible_job_id": "389800423634.24379", "changed": true, "finished": 0, "results_file": "/users/cseas002/.ansible_async/389800423634.24379", "started": 1}
changed: [node1] => {"ansible_job_id": "784007395171.26207", "changed": true, "finished": 0, "results_file": "/users/cseas002/.ansible_async/784007395171.26207", "started": 1}

PLAY [Check Status of run] *****************************************************

PLAY [Leave Swarm] *************************************************************

PLAY RECAP *********************************************************************
node1                      : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
node2                      : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

Using /users/cseas002/HDSearch-Multinode/ansible.cfg as config file

PLAY [Set uncore frequency] ****************************************************

TASK [Gathering Facts] *********************************************************
ok: [node2]
ok: [node1]

TASK [Load msr kernel module] **************************************************
changed: [node2] => {"changed": true, "cmd": "modprobe msr", "delta": "0:00:00.008023", "end": "2023-10-10 12:52:02.108347", "rc": 0, "start": "2023-10-10 12:52:02.100324", "stderr": "", "stderr_lines": [], "stdout": "", "stdout_lines": []}
changed: [node1] => {"changed": true, "cmd": "modprobe msr", "delta": "0:00:00.007923", "end": "2023-10-10 12:52:02.112869", "rc": 0, "start": "2023-10-10 12:52:02.104946", "stderr": "", "stderr_lines": [], "stdout": "", "stdout_lines": []}

TASK [Set uncore freq] *********************************************************
changed: [node2] => {"changed": true, "cmd": "wrmsr -p0 0x620 0x1414", "delta": "0:00:00.006266", "end": "2023-10-10 12:52:02.397226", "rc": 0, "start": "2023-10-10 12:52:02.390960", "stderr": "", "stderr_lines": [], "stdout": "", "stdout_lines": []}
changed: [node1] => {"changed": true, "cmd": "wrmsr -p0 0x620 0x1414", "delta": "0:00:00.006687", "end": "2023-10-10 12:52:02.433234", "rc": 0, "start": "2023-10-10 12:52:02.426547", "stderr": "", "stderr_lines": [], "stdout": "", "stdout_lines": []}

PLAY RECAP *********************************************************************
node1                      : ok=3    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
node2                      : ok=3    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

Nothing found in stack: microsuite
Using /users/cseas002/HDSearch-Multinode/ansible.cfg as config file
[WARNING]: While constructing a mapping from /users/cseas002/HDSearch-
Multinode/ansible/profiler.yml, line 20, column 3, found a duplicate dict key
(hosts). Using last defined value only.
[WARNING]: While constructing a mapping from /users/cseas002/HDSearch-
Multinode/ansible/profiler.yml, line 20, column 3, found a duplicate dict key
(tags). Using last defined value only.
[WARNING]: While constructing a mapping from /users/cseas002/HDSearch-
Multinode/ansible/profiler.yml, line 20, column 3, found a duplicate dict key
(tasks). Using last defined value only.

PLAY [Set profiler] ************************************************************

PLAY [Run remote profiler] *****************************************************

PLAY [Kill remote profiler] ****************************************************

PLAY RECAP *********************************************************************

INFO:root:ssh -A node0 hostname
INFO:root:node0.hdsearch2.ramp-pg0.wisc.cloudlab.us
INFO:root:ssh -A node1 hostname
INFO:root:node1.hdsearch2.ramp-pg0.wisc.cloudlab.us
INFO:root:ssh -A node2 hostname
INFO:root:node2.hdsearch2.ramp-pg0.wisc.cloudlab.us
Ignoring deprecated options:

expose: Exposing ports is unnecessary - services on the same network can access each other's containers on any port.

Creating network microsuite_default
Creating service microsuite_midtier
Creating service microsuite_client
Creating service microsuite_bucket
Using /users/cseas002/HDSearch-Multinode/ansible.cfg as config file
[WARNING]: While constructing a mapping from /users/cseas002/HDSearch-
Multinode/ansible/profiler.yml, line 20, column 3, found a duplicate dict key
(hosts). Using last defined value only.
[WARNING]: While constructing a mapping from /users/cseas002/HDSearch-
Multinode/ansible/profiler.yml, line 20, column 3, found a duplicate dict key
(tags). Using last defined value only.
[WARNING]: While constructing a mapping from /users/cseas002/HDSearch-
Multinode/ansible/profiler.yml, line 20, column 3, found a duplicate dict key
(tasks). Using last defined value only.

PLAY [Set profiler] ************************************************************

PLAY [Run remote profiler] *****************************************************

PLAY [Kill remote profiler] ****************************************************

TASK [Gathering Facts] *********************************************************
ok: [node2]
ok: [node1]

TASK [Run remote socwatch] *****************************************************
changed: [node2] => {"ansible_job_id": "887338241365.24855", "changed": true, "finished": 0, "results_file": "/users/cseas002/.ansible_async/887338241365.24855", "started": 1}
changed: [node1] => {"ansible_job_id": "1974298624.26695", "changed": true, "finished": 0, "results_file": "/users/cseas002/.ansible_async/1974298624.26695", "started": 1}

PLAY RECAP *********************************************************************
node1                      : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
node2                      : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

Using /users/cseas002/HDSearch-Multinode/ansible.cfg as config file

PLAY [Initialize cluster manager] **********************************************

PLAY [Initialize workers] ******************************************************

PLAY [Check Status of run] *****************************************************

TASK [Gathering Facts] *********************************************************
ok: [node0]

TASK [Check Status] ************************************************************
changed: [node0] => {"changed": true, "cmd": "/users/cseas002/HDSearch-Multinode/scripts/check-run-status.sh", "delta": "1:33:11.304644", "end": "2023-10-10 14:25:39.174426", "rc": 0, "start": "2023-10-10 12:52:27.869782", "stderr": "Error response from daemon: This node is not a swarm manager. Use \"docker swarm init\" or \"docker swarm join\" to connect this node to swarm and try again.\nError response from daemon: This node is not a swarm manager. Use \"docker swarm init\" or \"docker swarm join\" to connect this node to swarm and try again.\nError response from daemon: This node is not a swarm manager. Use \"docker swarm init\" or \"docker swarm join\" to connect this node to swarm and try again.\nError response from daemon: This node is not a swarm manager. Use \"docker swarm init\" or \"docker swarm join\" to connect this node to swarm and try again.\nError response from daemon: This node is not a swarm manager. Use \"docker swarm init\" or \"docker swarm join\" to connect this node to swarm and try again.\nError response from daemon: This node is not a swarm manager. Use \"docker swarm init\" or \"docker swarm join\" to connect this node to swarm and try again.\nError response from daemon: This node is not a swarm manager. Use \"docker swarm init\" or \"docker swarm join\" to connect this node to swarm and try again.\nError response from daemon: This node is not a swarm manager. Use \"docker swarm init\" or \"docker swarm join\" to connect this node to swarm and try again.\nError response from daemon: This node is not a swarm manager. Use \"docker swarm init\" or \"docker swarm join\" to connect this node to swarm and try again.\nError response from daemon: This node is not a swarm manager. Use \"docker swarm init\" or \"docker swarm join\" to connect this node to swarm and try again.\nError response from daemon: This node is not a swarm manager. Use \"docker swarm init\" or \"docker swarm join\" to connect this node to swarm and try again.\nError response from daemon: This node is not a swarm manager. Use \"docker swarm init\" or \"docker swarm join\" to connect this node to swarm and try again.\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client", "stderr_lines": ["Error response from daemon: This node is not a swarm manager. Use \"docker swarm init\" or \"docker swarm join\" to connect this node to swarm and try again.", "Error response from daemon: This node is not a swarm manager. Use \"docker swarm init\" or \"docker swarm join\" to connect this node to swarm and try again.", "Error response from daemon: This node is not a swarm manager. Use \"docker swarm init\" or \"docker swarm join\" to connect this node to swarm and try again.", "Error response from daemon: This node is not a swarm manager. Use \"docker swarm init\" or \"docker swarm join\" to connect this node to swarm and try again.", "Error response from daemon: This node is not a swarm manager. Use \"docker swarm init\" or \"docker swarm join\" to connect this node to swarm and try again.", "Error response from daemon: This node is not a swarm manager. Use \"docker swarm init\" or \"docker swarm join\" to connect this node to swarm and try again.", "Error response from daemon: This node is not a swarm manager. Use \"docker swarm init\" or \"docker swarm join\" to connect this node to swarm and try again.", "Error response from daemon: This node is not a swarm manager. Use \"docker swarm init\" or \"docker swarm join\" to connect this node to swarm and try again.", "Error response from daemon: This node is not a swarm manager. Use \"docker swarm init\" or \"docker swarm join\" to connect this node to swarm and try again.", "Error response from daemon: This node is not a swarm manager. Use \"docker swarm init\" or \"docker swarm join\" to connect this node to swarm and try again.", "Error response from daemon: This node is not a swarm manager. Use \"docker swarm init\" or \"docker swarm join\" to connect this node to swarm and try again.", "Error response from daemon: This node is not a swarm manager. Use \"docker swarm init\" or \"docker swarm join\" to connect this node to swarm and try again.", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client"], "stdout": "", "stdout_lines": []}

PLAY [Leave Swarm] *************************************************************

PLAY RECAP *********************************************************************
node0                      : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

