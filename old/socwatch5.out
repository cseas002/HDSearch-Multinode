Using /users/cseas002/HDSearch-Multinode/ansible.cfg as config file
[WARNING]: While constructing a mapping from /users/cseas002/HDSearch-
Multinode/ansible/profiler.yml, line 20, column 3, found a duplicate dict key
(hosts). Using last defined value only.
[WARNING]: While constructing a mapping from /users/cseas002/HDSearch-
Multinode/ansible/profiler.yml, line 20, column 3, found a duplicate dict key
(tags). Using last defined value only.
[WARNING]: While constructing a mapping from /users/cseas002/HDSearch-
Multinode/ansible/profiler.yml, line 20, column 3, found a duplicate dict key
(tasks). Using last defined value only.

PLAY [Set profiler] ************************************************************

TASK [Gathering Facts] *********************************************************
ok: [node1]
ok: [node2]

TASK [Set profiler Hosts] ******************************************************
changed: [node2] => {"ansible_job_id": "933732263716.8644", "changed": true, "finished": 0, "results_file": "/users/cseas002/.ansible_async/933732263716.8644", "started": 1}
changed: [node1] => {"ansible_job_id": "703324515783.17581", "changed": true, "finished": 0, "results_file": "/users/cseas002/.ansible_async/703324515783.17581", "started": 1}

PLAY [Run remote profiler] *****************************************************

PLAY [Kill remote profiler] ****************************************************

PLAY RECAP *********************************************************************
node1                      : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
node2                      : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

Using /users/cseas002/HDSearch-Multinode/ansible.cfg as config file

PLAY [Initialize cluster manager] **********************************************

PLAY [Initialize workers] ******************************************************

PLAY [Check Status of run] *****************************************************

PLAY [Leave Swarm] *************************************************************

TASK [Gathering Facts] *********************************************************
ok: [node1]
ok: [node2]
ok: [node0]

TASK [Leave Swarm] *************************************************************
changed: [node2] => {"ansible_job_id": "25719064733.8761", "changed": true, "finished": 0, "results_file": "/users/cseas002/.ansible_async/25719064733.8761", "started": 1}
changed: [node1] => {"ansible_job_id": "808598088763.17698", "changed": true, "finished": 0, "results_file": "/users/cseas002/.ansible_async/808598088763.17698", "started": 1}
changed: [node0] => {"ansible_job_id": "336581307307.1992", "changed": true, "finished": 0, "results_file": "/users/cseas002/.ansible_async/336581307307.1992", "started": 1}

PLAY RECAP *********************************************************************
node0                      : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
node1                      : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
node2                      : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

Using /users/cseas002/HDSearch-Multinode/ansible.cfg as config file

PLAY [Initialize cluster manager] **********************************************

TASK [Gathering Facts] *********************************************************
ok: [node0]

TASK [Init Master] *************************************************************
[WARNING]: Consider using 'become', 'become_method', and 'become_user' rather
than running sudo
changed: [node0] => {"changed": true, "cmd": "sudo docker swarm init --advertise-addr 10.10.1.1", "delta": "0:00:00.608028", "end": "2023-09-18 14:44:43.176189", "rc": 0, "start": "2023-09-18 14:44:42.568161", "stderr": "", "stderr_lines": [], "stdout": "Swarm initialized: current node (zaskdaaxcct3yaahzmec9wk8b) is now a manager.\n\nTo add a worker to this swarm, run the following command:\n\n    docker swarm join --token SWMTKN-1-34ldsk0g3o32o7mbdpwct5ww9oppwsfr1kp8qluqm1b9uysi3d-d4zuwftssjkdutuh2ezqhkjis 10.10.1.1:2377\n\nTo add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.", "stdout_lines": ["Swarm initialized: current node (zaskdaaxcct3yaahzmec9wk8b) is now a manager.", "", "To add a worker to this swarm, run the following command:", "", "    docker swarm join --token SWMTKN-1-34ldsk0g3o32o7mbdpwct5ww9oppwsfr1kp8qluqm1b9uysi3d-d4zuwftssjkdutuh2ezqhkjis 10.10.1.1:2377", "", "To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions."]}

PLAY [Initialize workers] ******************************************************

PLAY [Check Status of run] *****************************************************

PLAY [Leave Swarm] *************************************************************

PLAY RECAP *********************************************************************
node0                      : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

Using /users/cseas002/HDSearch-Multinode/ansible.cfg as config file

PLAY [Initialize cluster manager] **********************************************

PLAY [Initialize workers] ******************************************************

TASK [Gathering Facts] *********************************************************
ok: [node1]
ok: [node2]

TASK [Init Workers] ************************************************************
changed: [node1] => {"ansible_job_id": "323260506308.17857", "changed": true, "finished": 0, "results_file": "/users/cseas002/.ansible_async/323260506308.17857", "started": 1}
changed: [node2] => {"ansible_job_id": "806557992780.8918", "changed": true, "finished": 0, "results_file": "/users/cseas002/.ansible_async/806557992780.8918", "started": 1}

PLAY [Check Status of run] *****************************************************

PLAY [Leave Swarm] *************************************************************

PLAY RECAP *********************************************************************
node1                      : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
node2                      : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

Using /users/cseas002/HDSearch-Multinode/ansible.cfg as config file

PLAY [Set uncore frequency] ****************************************************

TASK [Gathering Facts] *********************************************************
ok: [node2]
ok: [node1]

TASK [Load msr kernel module] **************************************************
changed: [node2] => {"changed": true, "cmd": "modprobe msr", "delta": "0:00:00.004303", "end": "2023-09-18 14:44:48.088403", "rc": 0, "start": "2023-09-18 14:44:48.084100", "stderr": "", "stderr_lines": [], "stdout": "", "stdout_lines": []}
changed: [node1] => {"changed": true, "cmd": "modprobe msr", "delta": "0:00:00.004336", "end": "2023-09-18 14:44:48.118999", "rc": 0, "start": "2023-09-18 14:44:48.114663", "stderr": "", "stderr_lines": [], "stdout": "", "stdout_lines": []}

TASK [Set uncore freq] *********************************************************
changed: [node2] => {"changed": true, "cmd": "wrmsr -p0 0x620 0x1414", "delta": "0:00:00.003981", "end": "2023-09-18 14:44:48.386656", "rc": 0, "start": "2023-09-18 14:44:48.382675", "stderr": "", "stderr_lines": [], "stdout": "", "stdout_lines": []}
changed: [node1] => {"changed": true, "cmd": "wrmsr -p0 0x620 0x1414", "delta": "0:00:00.003693", "end": "2023-09-18 14:44:48.403621", "rc": 0, "start": "2023-09-18 14:44:48.399928", "stderr": "", "stderr_lines": [], "stdout": "", "stdout_lines": []}

PLAY RECAP *********************************************************************
node1                      : ok=3    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
node2                      : ok=3    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

Nothing found in stack: microsuite
Using /users/cseas002/HDSearch-Multinode/ansible.cfg as config file
[WARNING]: While constructing a mapping from /users/cseas002/HDSearch-
Multinode/ansible/profiler.yml, line 20, column 3, found a duplicate dict key
(hosts). Using last defined value only.
[WARNING]: While constructing a mapping from /users/cseas002/HDSearch-
Multinode/ansible/profiler.yml, line 20, column 3, found a duplicate dict key
(tags). Using last defined value only.
[WARNING]: While constructing a mapping from /users/cseas002/HDSearch-
Multinode/ansible/profiler.yml, line 20, column 3, found a duplicate dict key
(tasks). Using last defined value only.

PLAY [Set profiler] ************************************************************

PLAY [Run remote profiler] *****************************************************

PLAY [Kill remote profiler] ****************************************************

PLAY RECAP *********************************************************************

INFO:root:ssh -A node0 hostname
INFO:root:node0.hdsearch2.ramp-pg0.wisc.cloudlab.us
INFO:root:ssh -A node1 hostname
INFO:root:node1.hdsearch2.ramp-pg0.wisc.cloudlab.us
INFO:root:ssh -A node2 hostname
INFO:root:node2.hdsearch2.ramp-pg0.wisc.cloudlab.us
Ignoring deprecated options:

expose: Exposing ports is unnecessary - services on the same network can access each other's containers on any port.

Creating network microsuite_default
Creating service microsuite_client
Creating service microsuite_bucket
Creating service microsuite_midtier
Using /users/cseas002/HDSearch-Multinode/ansible.cfg as config file
[WARNING]: While constructing a mapping from /users/cseas002/HDSearch-
Multinode/ansible/profiler.yml, line 20, column 3, found a duplicate dict key
(hosts). Using last defined value only.
[WARNING]: While constructing a mapping from /users/cseas002/HDSearch-
Multinode/ansible/profiler.yml, line 20, column 3, found a duplicate dict key
(tags). Using last defined value only.
[WARNING]: While constructing a mapping from /users/cseas002/HDSearch-
Multinode/ansible/profiler.yml, line 20, column 3, found a duplicate dict key
(tasks). Using last defined value only.

PLAY [Set profiler] ************************************************************

PLAY [Run remote profiler] *****************************************************

TASK [Gathering Facts] *********************************************************
ok: [node0]

TASK [Run remote profiler] *****************************************************
changed: [node0] => {"changed": true, "cmd": "/users/cseas002/HDSearch-Multinode/scripts/startProfiler.sh /users/cseas002/HDSearch-Multinode/hosts 0", "delta": "0:21:21.719362", "end": "2023-09-18 15:06:33.665148", "rc": 0, "start": "2023-09-18 14:45:11.945786", "stderr": "Error response from daemon: This node is not a swarm manager. Use \"docker swarm init\" or \"docker swarm join\" to connect this node to swarm and try again.\nError response from daemon: This node is not a swarm manager. Use \"docker swarm init\" or \"docker swarm join\" to connect this node to swarm and try again.\nError response from daemon: This node is not a swarm manager. Use \"docker swarm init\" or \"docker swarm join\" to connect this node to swarm and try again.\nError response from daemon: This node is not a swarm manager. Use \"docker swarm init\" or \"docker swarm join\" to connect this node to swarm and try again.\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\nno such task or service: microsuite_client\n[DEPRECATION WARNING]: Distribution Ubuntu 18.04 on host node2 should use \n/usr/bin/python3, but is using /usr/bin/python for backward compatibility with \nprior Ansible releases. A future Ansible release will default to using the \ndiscovered platform python for this host. See https://docs.ansible.com/ansible/\n2.9/reference_appendices/interpreter_discovery.html for more information. This \nfeature will be removed in version 2.12. Deprecation warnings can be disabled \nby setting deprecation_warnings=False in ansible.cfg.\n[DEPRECATION WARNING]: Distribution Ubuntu 18.04 on host node1 should use \n/usr/bin/python3, but is using /usr/bin/python for backward compatibility with \nprior Ansible releases. A future Ansible release will default to using the \ndiscovered platform python for this host. See https://docs.ansible.com/ansible/\n2.9/reference_appendices/interpreter_discovery.html for more information. This \nfeature will be removed in version 2.12. Deprecation warnings can be disabled \nby setting deprecation_warnings=False in ansible.cfg.\nPseudo-terminal will not be allocated because stdin is not a terminal.\r\nssh: Could not resolve hostname cd /users/cseas002/hdsearch-multinode/profiler/; sudo /users/cseas002/hdsearch-multinode/profiler/profiler.sh run_profiler 0 : Name or service not known", "stderr_lines": ["Error response from daemon: This node is not a swarm manager. Use \"docker swarm init\" or \"docker swarm join\" to connect this node to swarm and try again.", "Error response from daemon: This node is not a swarm manager. Use \"docker swarm init\" or \"docker swarm join\" to connect this node to swarm and try again.", "Error response from daemon: This node is not a swarm manager. Use \"docker swarm init\" or \"docker swarm join\" to connect this node to swarm and try again.", "Error response from daemon: This node is not a swarm manager. Use \"docker swarm init\" or \"docker swarm join\" to connect this node to swarm and try again.", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "no such task or service: microsuite_client", "[DEPRECATION WARNING]: Distribution Ubuntu 18.04 on host node2 should use ", "/usr/bin/python3, but is using /usr/bin/python for backward compatibility with ", "prior Ansible releases. A future Ansible release will default to using the ", "discovered platform python for this host. See https://docs.ansible.com/ansible/", "2.9/reference_appendices/interpreter_discovery.html for more information. This ", "feature will be removed in version 2.12. Deprecation warnings can be disabled ", "by setting deprecation_warnings=False in ansible.cfg.", "[DEPRECATION WARNING]: Distribution Ubuntu 18.04 on host node1 should use ", "/usr/bin/python3, but is using /usr/bin/python for backward compatibility with ", "prior Ansible releases. A future Ansible release will default to using the ", "discovered platform python for this host. See https://docs.ansible.com/ansible/", "2.9/reference_appendices/interpreter_discovery.html for more information. This ", "feature will be removed in version 2.12. Deprecation warnings can be disabled ", "by setting deprecation_warnings=False in ansible.cfg.", "Pseudo-terminal will not be allocated because stdin is not a terminal.", "ssh: Could not resolve hostname cd /users/cseas002/hdsearch-multinode/profiler/; sudo /users/cseas002/hdsearch-multinode/profiler/profiler.sh run_profiler 0 : Name or service not known"], "stdout": "/users/cseas002/HDSearch-Multinode/hosts\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nnode1\n11598\nUsing /etc/ansible/ansible.cfg as config file\n\nPLAY [Run remote profiler] *****************************************************\n\nTASK [Gathering Facts] *********************************************************\nok: [node2]\n\nTASK [Run remote profiler] *****************************************************\nchanged: [node2] => {\"ansible_job_id\": \"963306970085.12768\", \"changed\": true, \"finished\": 0, \"results_file\": \"/root/.ansible_async/963306970085.12768\", \"started\": 1}\n\nTASK [Check remote profiler] ***************************************************\nfatal: [node2]: FAILED! => {\"ansible_job_id\": \"963306970085.12768\", \"changed\": true, \"cmd\": [\"python3\", \"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\", \"-i\", \"0\", \"-t\", \"11598\"], \"delta\": \"0:00:00.121556\", \"end\": \"2023-09-18 15:06:29.033490\", \"finished\": 1, \"msg\": \"non-zero return code\", \"rc\": 1, \"start\": \"2023-09-18 15:06:28.911934\", \"stderr\": \"Traceback (most recent call last):\\n  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 563, in <module>\\n    main()\\n  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 554, in main\\n    real_main()\\n  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 551, in real_main\\n    parse_args()\\n  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 548, in parse_args\\n    server(args.port,args.perf_iteration,args.process_id)\\n  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 433, in server\\n    server = SimpleXMLRPCServer((hostname, port), allow_none=True)\\n  File \\\"/usr/lib/python3.6/xmlrpc/server.py\\\", line 599, in __init__\\n    socketserver.TCPServer.__init__(self, addr, requestHandler, bind_and_activate)\\n  File \\\"/usr/lib/python3.6/socketserver.py\\\", line 456, in __init__\\n    self.server_bind()\\n  File \\\"/usr/lib/python3.6/socketserver.py\\\", line 470, in server_bind\\n    self.socket.bind(self.server_address)\\nOSError: [Errno 98] Address already in use\", \"stderr_lines\": [\"Traceback (most recent call last):\", \"  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 563, in <module>\", \"    main()\", \"  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 554, in main\", \"    real_main()\", \"  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 551, in real_main\", \"    parse_args()\", \"  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 548, in parse_args\", \"    server(args.port,args.perf_iteration,args.process_id)\", \"  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 433, in server\", \"    server = SimpleXMLRPCServer((hostname, port), allow_none=True)\", \"  File \\\"/usr/lib/python3.6/xmlrpc/server.py\\\", line 599, in __init__\", \"    socketserver.TCPServer.__init__(self, addr, requestHandler, bind_and_activate)\", \"  File \\\"/usr/lib/python3.6/socketserver.py\\\", line 456, in __init__\", \"    self.server_bind()\", \"  File \\\"/usr/lib/python3.6/socketserver.py\\\", line 470, in server_bind\", \"    self.socket.bind(self.server_address)\", \"OSError: [Errno 98] Address already in use\"], \"stdout\": \"\", \"stdout_lines\": []}\n\nPLAY RECAP *********************************************************************\nnode2                      : ok=2    changed=1    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   \n\nnode2\nnode1\nUsing /etc/ansible/ansible.cfg as config file\n\nPLAY [Run remote profiler] *****************************************************\n\nTASK [Gathering Facts] *********************************************************\nok: [node1]\n\nTASK [Run remote profiler] *****************************************************\nchanged: [node1] => {\"ansible_job_id\": \"228948821285.22138\", \"changed\": true, \"finished\": 0, \"results_file\": \"/root/.ansible_async/228948821285.22138\", \"started\": 1}\n\nTASK [Check remote profiler] ***************************************************\nfatal: [node1]: FAILED! => {\"ansible_job_id\": \"228948821285.22138\", \"changed\": true, \"cmd\": [\"python3\", \"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\", \"-i\", \"0\", \"-t\", \"21154\"], \"delta\": \"0:00:00.125276\", \"end\": \"2023-09-18 15:06:33.018796\", \"finished\": 1, \"msg\": \"non-zero return code\", \"rc\": 1, \"start\": \"2023-09-18 15:06:32.893520\", \"stderr\": \"Traceback (most recent call last):\\n  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 563, in <module>\\n    main()\\n  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 554, in main\\n    real_main()\\n  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 551, in real_main\\n    parse_args()\\n  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 548, in parse_args\\n    server(args.port,args.perf_iteration,args.process_id)\\n  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 433, in server\\n    server = SimpleXMLRPCServer((hostname, port), allow_none=True)\\n  File \\\"/usr/lib/python3.6/xmlrpc/server.py\\\", line 599, in __init__\\n    socketserver.TCPServer.__init__(self, addr, requestHandler, bind_and_activate)\\n  File \\\"/usr/lib/python3.6/socketserver.py\\\", line 456, in __init__\\n    self.server_bind()\\n  File \\\"/usr/lib/python3.6/socketserver.py\\\", line 470, in server_bind\\n    self.socket.bind(self.server_address)\\nOSError: [Errno 98] Address already in use\", \"stderr_lines\": [\"Traceback (most recent call last):\", \"  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 563, in <module>\", \"    main()\", \"  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 554, in main\", \"    real_main()\", \"  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 551, in real_main\", \"    parse_args()\", \"  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 548, in parse_args\", \"    server(args.port,args.perf_iteration,args.process_id)\", \"  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 433, in server\", \"    server = SimpleXMLRPCServer((hostname, port), allow_none=True)\", \"  File \\\"/usr/lib/python3.6/xmlrpc/server.py\\\", line 599, in __init__\", \"    socketserver.TCPServer.__init__(self, addr, requestHandler, bind_and_activate)\", \"  File \\\"/usr/lib/python3.6/socketserver.py\\\", line 456, in __init__\", \"    self.server_bind()\", \"  File \\\"/usr/lib/python3.6/socketserver.py\\\", line 470, in server_bind\", \"    self.socket.bind(self.server_address)\", \"OSError: [Errno 98] Address already in use\"], \"stdout\": \"\", \"stdout_lines\": []}\n\nPLAY RECAP *********************************************************************\nnode1                      : ok=2    changed=1    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   \n\n\nnode2", "stdout_lines": ["/users/cseas002/HDSearch-Multinode/hosts", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "node1", "11598", "Using /etc/ansible/ansible.cfg as config file", "", "PLAY [Run remote profiler] *****************************************************", "", "TASK [Gathering Facts] *********************************************************", "ok: [node2]", "", "TASK [Run remote profiler] *****************************************************", "changed: [node2] => {\"ansible_job_id\": \"963306970085.12768\", \"changed\": true, \"finished\": 0, \"results_file\": \"/root/.ansible_async/963306970085.12768\", \"started\": 1}", "", "TASK [Check remote profiler] ***************************************************", "fatal: [node2]: FAILED! => {\"ansible_job_id\": \"963306970085.12768\", \"changed\": true, \"cmd\": [\"python3\", \"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\", \"-i\", \"0\", \"-t\", \"11598\"], \"delta\": \"0:00:00.121556\", \"end\": \"2023-09-18 15:06:29.033490\", \"finished\": 1, \"msg\": \"non-zero return code\", \"rc\": 1, \"start\": \"2023-09-18 15:06:28.911934\", \"stderr\": \"Traceback (most recent call last):\\n  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 563, in <module>\\n    main()\\n  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 554, in main\\n    real_main()\\n  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 551, in real_main\\n    parse_args()\\n  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 548, in parse_args\\n    server(args.port,args.perf_iteration,args.process_id)\\n  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 433, in server\\n    server = SimpleXMLRPCServer((hostname, port), allow_none=True)\\n  File \\\"/usr/lib/python3.6/xmlrpc/server.py\\\", line 599, in __init__\\n    socketserver.TCPServer.__init__(self, addr, requestHandler, bind_and_activate)\\n  File \\\"/usr/lib/python3.6/socketserver.py\\\", line 456, in __init__\\n    self.server_bind()\\n  File \\\"/usr/lib/python3.6/socketserver.py\\\", line 470, in server_bind\\n    self.socket.bind(self.server_address)\\nOSError: [Errno 98] Address already in use\", \"stderr_lines\": [\"Traceback (most recent call last):\", \"  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 563, in <module>\", \"    main()\", \"  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 554, in main\", \"    real_main()\", \"  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 551, in real_main\", \"    parse_args()\", \"  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 548, in parse_args\", \"    server(args.port,args.perf_iteration,args.process_id)\", \"  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 433, in server\", \"    server = SimpleXMLRPCServer((hostname, port), allow_none=True)\", \"  File \\\"/usr/lib/python3.6/xmlrpc/server.py\\\", line 599, in __init__\", \"    socketserver.TCPServer.__init__(self, addr, requestHandler, bind_and_activate)\", \"  File \\\"/usr/lib/python3.6/socketserver.py\\\", line 456, in __init__\", \"    self.server_bind()\", \"  File \\\"/usr/lib/python3.6/socketserver.py\\\", line 470, in server_bind\", \"    self.socket.bind(self.server_address)\", \"OSError: [Errno 98] Address already in use\"], \"stdout\": \"\", \"stdout_lines\": []}", "", "PLAY RECAP *********************************************************************", "node2                      : ok=2    changed=1    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   ", "", "node2", "node1", "Using /etc/ansible/ansible.cfg as config file", "", "PLAY [Run remote profiler] *****************************************************", "", "TASK [Gathering Facts] *********************************************************", "ok: [node1]", "", "TASK [Run remote profiler] *****************************************************", "changed: [node1] => {\"ansible_job_id\": \"228948821285.22138\", \"changed\": true, \"finished\": 0, \"results_file\": \"/root/.ansible_async/228948821285.22138\", \"started\": 1}", "", "TASK [Check remote profiler] ***************************************************", "fatal: [node1]: FAILED! => {\"ansible_job_id\": \"228948821285.22138\", \"changed\": true, \"cmd\": [\"python3\", \"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\", \"-i\", \"0\", \"-t\", \"21154\"], \"delta\": \"0:00:00.125276\", \"end\": \"2023-09-18 15:06:33.018796\", \"finished\": 1, \"msg\": \"non-zero return code\", \"rc\": 1, \"start\": \"2023-09-18 15:06:32.893520\", \"stderr\": \"Traceback (most recent call last):\\n  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 563, in <module>\\n    main()\\n  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 554, in main\\n    real_main()\\n  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 551, in real_main\\n    parse_args()\\n  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 548, in parse_args\\n    server(args.port,args.perf_iteration,args.process_id)\\n  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 433, in server\\n    server = SimpleXMLRPCServer((hostname, port), allow_none=True)\\n  File \\\"/usr/lib/python3.6/xmlrpc/server.py\\\", line 599, in __init__\\n    socketserver.TCPServer.__init__(self, addr, requestHandler, bind_and_activate)\\n  File \\\"/usr/lib/python3.6/socketserver.py\\\", line 456, in __init__\\n    self.server_bind()\\n  File \\\"/usr/lib/python3.6/socketserver.py\\\", line 470, in server_bind\\n    self.socket.bind(self.server_address)\\nOSError: [Errno 98] Address already in use\", \"stderr_lines\": [\"Traceback (most recent call last):\", \"  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 563, in <module>\", \"    main()\", \"  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 554, in main\", \"    real_main()\", \"  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 551, in real_main\", \"    parse_args()\", \"  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 548, in parse_args\", \"    server(args.port,args.perf_iteration,args.process_id)\", \"  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 433, in server\", \"    server = SimpleXMLRPCServer((hostname, port), allow_none=True)\", \"  File \\\"/usr/lib/python3.6/xmlrpc/server.py\\\", line 599, in __init__\", \"    socketserver.TCPServer.__init__(self, addr, requestHandler, bind_and_activate)\", \"  File \\\"/usr/lib/python3.6/socketserver.py\\\", line 456, in __init__\", \"    self.server_bind()\", \"  File \\\"/usr/lib/python3.6/socketserver.py\\\", line 470, in server_bind\", \"    self.socket.bind(self.server_address)\", \"OSError: [Errno 98] Address already in use\"], \"stdout\": \"\", \"stdout_lines\": []}", "", "PLAY RECAP *********************************************************************", "node1                      : ok=2    changed=1    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   ", "", "", "node2"]}

PLAY [Kill remote profiler] ****************************************************

PLAY RECAP *********************************************************************
node0                      : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

