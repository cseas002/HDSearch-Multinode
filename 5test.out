Using /users/cseas002/HDSearch-Multinode/ansible.cfg as config file

PLAY [Install HDSearch] ********************************************************

TASK [Gathering Facts] *********************************************************
ok: [node1]
ok: [node2]

PLAY [Set Up Docker Curl] ******************************************************

TASK [Gathering Facts] *********************************************************
ok: [node2]
ok: [node1]
ok: [node0]

PLAY [Make space to commit] ****************************************************

TASK [Gathering Facts] *********************************************************
ok: [node2]
ok: [node0]
ok: [node1]

TASK [Make Space to Commit Image] **********************************************
[WARNING]: Consider using 'become', 'become_method', and 'become_user' rather
than running sudo
changed: [node2] => {"changed": true, "cmd": ["sudo", "~/HDSearch-Multinode/scripts/change-storage-location-docker.sh"], "delta": "0:00:00.982445", "end": "2023-10-10 14:07:35.003168", "rc": 0, "start": "2023-10-10 14:07:34.020723", "stderr": "\"docker rm\" requires at least 1 argument.\nSee 'docker rm --help'.\n\nUsage:  docker rm [OPTIONS] CONTAINER [CONTAINER...]\n\nRemove one or more containers\n\"docker rmi\" requires at least 1 argument.\nSee 'docker rmi --help'.\n\nUsage:  docker rmi [OPTIONS] IMAGE [IMAGE...]\n\nRemove one or more images\nWarning: Stopping docker.service, but it can still be activated by:\n  docker.socket\nmkdir: cannot create directory â€˜/dev/mkdockerâ€™: File exists", "stderr_lines": ["\"docker rm\" requires at least 1 argument.", "See 'docker rm --help'.", "", "Usage:  docker rm [OPTIONS] CONTAINER [CONTAINER...]", "", "Remove one or more containers", "\"docker rmi\" requires at least 1 argument.", "See 'docker rmi --help'.", "", "Usage:  docker rmi [OPTIONS] IMAGE [IMAGE...]", "", "Remove one or more images", "Warning: Stopping docker.service, but it can still be activated by:", "  docker.socket", "mkdir: cannot create directory â€˜/dev/mkdockerâ€™: File exists"], "stdout": "", "stdout_lines": []}
changed: [node1] => {"changed": true, "cmd": ["sudo", "~/HDSearch-Multinode/scripts/change-storage-location-docker.sh"], "delta": "0:00:00.980659", "end": "2023-10-10 14:07:35.013405", "rc": 0, "start": "2023-10-10 14:07:34.032746", "stderr": "\"docker rm\" requires at least 1 argument.\nSee 'docker rm --help'.\n\nUsage:  docker rm [OPTIONS] CONTAINER [CONTAINER...]\n\nRemove one or more containers\n\"docker rmi\" requires at least 1 argument.\nSee 'docker rmi --help'.\n\nUsage:  docker rmi [OPTIONS] IMAGE [IMAGE...]\n\nRemove one or more images\nWarning: Stopping docker.service, but it can still be activated by:\n  docker.socket\nmkdir: cannot create directory â€˜/dev/mkdockerâ€™: File exists", "stderr_lines": ["\"docker rm\" requires at least 1 argument.", "See 'docker rm --help'.", "", "Usage:  docker rm [OPTIONS] CONTAINER [CONTAINER...]", "", "Remove one or more containers", "\"docker rmi\" requires at least 1 argument.", "See 'docker rmi --help'.", "", "Usage:  docker rmi [OPTIONS] IMAGE [IMAGE...]", "", "Remove one or more images", "Warning: Stopping docker.service, but it can still be activated by:", "  docker.socket", "mkdir: cannot create directory â€˜/dev/mkdockerâ€™: File exists"], "stdout": "", "stdout_lines": []}
changed: [node0] => {"changed": true, "cmd": ["sudo", "~/HDSearch-Multinode/scripts/change-storage-location-docker.sh"], "delta": "0:00:05.093660", "end": "2023-10-10 14:07:39.110902", "rc": 0, "start": "2023-10-10 14:07:34.017242", "stderr": "\"docker rm\" requires at least 1 argument.\nSee 'docker rm --help'.\n\nUsage:  docker rm [OPTIONS] CONTAINER [CONTAINER...]\n\nRemove one or more containers\n\"docker rmi\" requires at least 1 argument.\nSee 'docker rmi --help'.\n\nUsage:  docker rmi [OPTIONS] IMAGE [IMAGE...]\n\nRemove one or more images\nWarning: Stopping docker.service, but it can still be activated by:\n  docker.socket\nmkdir: cannot create directory â€˜/dev/mkdockerâ€™: File exists", "stderr_lines": ["\"docker rm\" requires at least 1 argument.", "See 'docker rm --help'.", "", "Usage:  docker rm [OPTIONS] CONTAINER [CONTAINER...]", "", "Remove one or more containers", "\"docker rmi\" requires at least 1 argument.", "See 'docker rmi --help'.", "", "Usage:  docker rmi [OPTIONS] IMAGE [IMAGE...]", "", "Remove one or more images", "Warning: Stopping docker.service, but it can still be activated by:", "  docker.socket", "mkdir: cannot create directory â€˜/dev/mkdockerâ€™: File exists"], "stdout": "", "stdout_lines": []}

PLAY [Install Profiler Dep] ****************************************************

PLAY RECAP *********************************************************************
node0                      : ok=3    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
node1                      : ok=4    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
node2                      : ok=4    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

Using /users/cseas002/HDSearch-Multinode/ansible.cfg as config file
[WARNING]: While constructing a mapping from /users/cseas002/HDSearch-
Multinode/ansible/profiler.yml, line 20, column 3, found a duplicate dict key
(hosts). Using last defined value only.
[WARNING]: While constructing a mapping from /users/cseas002/HDSearch-
Multinode/ansible/profiler.yml, line 20, column 3, found a duplicate dict key
(tags). Using last defined value only.
[WARNING]: While constructing a mapping from /users/cseas002/HDSearch-
Multinode/ansible/profiler.yml, line 20, column 3, found a duplicate dict key
(tasks). Using last defined value only.

PLAY [Set profiler] ************************************************************

TASK [Gathering Facts] *********************************************************
ok: [node2]
ok: [node1]

TASK [Set profiler Hosts] ******************************************************
changed: [node2] => {"ansible_job_id": "605610946351.7721", "changed": true, "finished": 0, "results_file": "/users/cseas002/.ansible_async/605610946351.7721", "started": 1}
changed: [node1] => {"ansible_job_id": "105216729332.8288", "changed": true, "finished": 0, "results_file": "/users/cseas002/.ansible_async/105216729332.8288", "started": 1}

PLAY [Run remote profiler] *****************************************************

PLAY [Kill remote profiler] ****************************************************

PLAY RECAP *********************************************************************
node1                      : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
node2                      : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

Using /users/cseas002/HDSearch-Multinode/ansible.cfg as config file

PLAY [Initialize cluster manager] **********************************************

PLAY [Initialize workers] ******************************************************

PLAY [Check Status of run] *****************************************************

PLAY [Leave Swarm] *************************************************************

TASK [Gathering Facts] *********************************************************
ok: [node0]
ok: [node1]
ok: [node2]

TASK [Leave Swarm] *************************************************************
changed: [node2] => {"ansible_job_id": "736170787312.7847", "changed": true, "finished": 0, "results_file": "/users/cseas002/.ansible_async/736170787312.7847", "started": 1}
changed: [node0] => {"ansible_job_id": "891505369069.40833", "changed": true, "finished": 0, "results_file": "/users/cseas002/.ansible_async/891505369069.40833", "started": 1}
changed: [node1] => {"ansible_job_id": "780335788674.8415", "changed": true, "finished": 0, "results_file": "/users/cseas002/.ansible_async/780335788674.8415", "started": 1}

PLAY RECAP *********************************************************************
node0                      : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
node1                      : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
node2                      : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

Using /users/cseas002/HDSearch-Multinode/ansible.cfg as config file

PLAY [Initialize cluster manager] **********************************************

TASK [Gathering Facts] *********************************************************
ok: [node0]

TASK [Init Master] *************************************************************
[WARNING]: Consider using 'become', 'become_method', and 'become_user' rather
than running sudo
changed: [node0] => {"changed": true, "cmd": "sudo docker swarm init --advertise-addr 10.10.1.1", "delta": "0:00:00.593607", "end": "2023-10-10 14:07:51.346954", "rc": 0, "start": "2023-10-10 14:07:50.753347", "stderr": "", "stderr_lines": [], "stdout": "Swarm initialized: current node (m03qxo38fv8253s628al96i82) is now a manager.\n\nTo add a worker to this swarm, run the following command:\n\n    docker swarm join --token SWMTKN-1-079dk3jvivq3im3pm7tpvj671nfk2fd2mev0vyx0dm5c6mo7x0-8w9xe01tpfkhdkcqkqgqfe3o0 10.10.1.1:2377\n\nTo add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.", "stdout_lines": ["Swarm initialized: current node (m03qxo38fv8253s628al96i82) is now a manager.", "", "To add a worker to this swarm, run the following command:", "", "    docker swarm join --token SWMTKN-1-079dk3jvivq3im3pm7tpvj671nfk2fd2mev0vyx0dm5c6mo7x0-8w9xe01tpfkhdkcqkqgqfe3o0 10.10.1.1:2377", "", "To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions."]}

PLAY [Initialize workers] ******************************************************

PLAY [Check Status of run] *****************************************************

PLAY [Leave Swarm] *************************************************************

PLAY RECAP *********************************************************************
node0                      : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

Using /users/cseas002/HDSearch-Multinode/ansible.cfg as config file

PLAY [Initialize cluster manager] **********************************************

PLAY [Initialize workers] ******************************************************

TASK [Gathering Facts] *********************************************************
ok: [node2]
ok: [node1]

TASK [Init Workers] ************************************************************
changed: [node1] => {"ansible_job_id": "440650697971.8543", "changed": true, "finished": 0, "results_file": "/users/cseas002/.ansible_async/440650697971.8543", "started": 1}
changed: [node2] => {"ansible_job_id": "447841439777.7974", "changed": true, "finished": 0, "results_file": "/users/cseas002/.ansible_async/447841439777.7974", "started": 1}

PLAY [Check Status of run] *****************************************************

PLAY [Leave Swarm] *************************************************************

PLAY RECAP *********************************************************************
node1                      : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
node2                      : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

Using /users/cseas002/HDSearch-Multinode/ansible.cfg as config file

PLAY [Set uncore frequency] ****************************************************

TASK [Gathering Facts] *********************************************************
ok: [node2]
ok: [node1]

TASK [Load msr kernel module] **************************************************
changed: [node1] => {"changed": true, "cmd": "modprobe msr", "delta": "0:00:00.007674", "end": "2023-10-10 14:07:58.313969", "rc": 0, "start": "2023-10-10 14:07:58.306295", "stderr": "", "stderr_lines": [], "stdout": "", "stdout_lines": []}
changed: [node2] => {"changed": true, "cmd": "modprobe msr", "delta": "0:00:00.007943", "end": "2023-10-10 14:07:58.304626", "rc": 0, "start": "2023-10-10 14:07:58.296683", "stderr": "", "stderr_lines": [], "stdout": "", "stdout_lines": []}

TASK [Set uncore freq] *********************************************************
changed: [node2] => {"changed": true, "cmd": "wrmsr -p0 0x620 0x1414", "delta": "0:00:00.007175", "end": "2023-10-10 14:07:58.593027", "rc": 0, "start": "2023-10-10 14:07:58.585852", "stderr": "", "stderr_lines": [], "stdout": "", "stdout_lines": []}
changed: [node1] => {"changed": true, "cmd": "wrmsr -p0 0x620 0x1414", "delta": "0:00:00.006976", "end": "2023-10-10 14:07:58.602460", "rc": 0, "start": "2023-10-10 14:07:58.595484", "stderr": "", "stderr_lines": [], "stdout": "", "stdout_lines": []}

PLAY RECAP *********************************************************************
node1                      : ok=3    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
node2                      : ok=3    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

Nothing found in stack: microsuite
Using /users/cseas002/HDSearch-Multinode/ansible.cfg as config file
[WARNING]: While constructing a mapping from /users/cseas002/HDSearch-
Multinode/ansible/profiler.yml, line 20, column 3, found a duplicate dict key
(hosts). Using last defined value only.
[WARNING]: While constructing a mapping from /users/cseas002/HDSearch-
Multinode/ansible/profiler.yml, line 20, column 3, found a duplicate dict key
(tags). Using last defined value only.
[WARNING]: While constructing a mapping from /users/cseas002/HDSearch-
Multinode/ansible/profiler.yml, line 20, column 3, found a duplicate dict key
(tasks). Using last defined value only.

PLAY [Set profiler] ************************************************************

PLAY [Run remote profiler] *****************************************************

PLAY [Kill remote profiler] ****************************************************

PLAY RECAP *********************************************************************

INFO:root:ssh -A node0 hostname
INFO:root:node0.hdsearch2.ramp-pg0.wisc.cloudlab.us
INFO:root:ssh -A node1 hostname
INFO:root:node1.hdsearch2.ramp-pg0.wisc.cloudlab.us
INFO:root:ssh -A node2 hostname
INFO:root:node2.hdsearch2.ramp-pg0.wisc.cloudlab.us
Ignoring deprecated options:

expose: Exposing ports is unnecessary - services on the same network can access each other's containers on any port.

Creating network microsuite_default
Creating service microsuite_bucket
Creating service microsuite_midtier
Creating service microsuite_client
Using /users/cseas002/HDSearch-Multinode/ansible.cfg as config file
[WARNING]: While constructing a mapping from /users/cseas002/HDSearch-
Multinode/ansible/profiler.yml, line 20, column 3, found a duplicate dict key
(hosts). Using last defined value only.
[WARNING]: While constructing a mapping from /users/cseas002/HDSearch-
Multinode/ansible/profiler.yml, line 20, column 3, found a duplicate dict key
(tags). Using last defined value only.
[WARNING]: While constructing a mapping from /users/cseas002/HDSearch-
Multinode/ansible/profiler.yml, line 20, column 3, found a duplicate dict key
(tasks). Using last defined value only.

PLAY [Set profiler] ************************************************************

PLAY [Run remote profiler] *****************************************************

TASK [Gathering Facts] *********************************************************
ok: [node0]

TASK [Run remote profiler] *****************************************************
changed: [node0] => {"changed": true, "cmd": "~/HDSearch-Multinode/scripts/startProfiler.sh /users/cseas002/HDSearch-Multinode/hosts 0", "delta": "0:15:30.044777", "end": "2023-10-10 14:23:50.717315", "rc": 0, "start": "2023-10-10 14:08:20.672538", "stderr": "[DEPRECATION WARNING]: Distribution Ubuntu 18.04 on host node2 should use \n/usr/bin/python3, but is using /usr/bin/python for backward compatibility with \nprior Ansible releases. A future Ansible release will default to using the \ndiscovered platform python for this host. See https://docs.ansible.com/ansible/\n2.9/reference_appendices/interpreter_discovery.html for more information. This \nfeature will be removed in version 2.12. Deprecation warnings can be disabled \nby setting deprecation_warnings=False in ansible.cfg.\n[DEPRECATION WARNING]: Distribution Ubuntu 18.04 on host node1 should use \n/usr/bin/python3, but is using /usr/bin/python for backward compatibility with \nprior Ansible releases. A future Ansible release will default to using the \ndiscovered platform python for this host. See https://docs.ansible.com/ansible/\n2.9/reference_appendices/interpreter_discovery.html for more information. This \nfeature will be removed in version 2.12. Deprecation warnings can be disabled \nby setting deprecation_warnings=False in ansible.cfg.\nPseudo-terminal will not be allocated because stdin is not a terminal.\r\nssh: Could not resolve hostname cd ~/hdsearch-multinode/profiler/; sudo ~/hdsearch-multinode/profiler/profiler.sh run_profiler 0 : Name or service not known", "stderr_lines": ["[DEPRECATION WARNING]: Distribution Ubuntu 18.04 on host node2 should use ", "/usr/bin/python3, but is using /usr/bin/python for backward compatibility with ", "prior Ansible releases. A future Ansible release will default to using the ", "discovered platform python for this host. See https://docs.ansible.com/ansible/", "2.9/reference_appendices/interpreter_discovery.html for more information. This ", "feature will be removed in version 2.12. Deprecation warnings can be disabled ", "by setting deprecation_warnings=False in ansible.cfg.", "[DEPRECATION WARNING]: Distribution Ubuntu 18.04 on host node1 should use ", "/usr/bin/python3, but is using /usr/bin/python for backward compatibility with ", "prior Ansible releases. A future Ansible release will default to using the ", "discovered platform python for this host. See https://docs.ansible.com/ansible/", "2.9/reference_appendices/interpreter_discovery.html for more information. This ", "feature will be removed in version 2.12. Deprecation warnings can be disabled ", "by setting deprecation_warnings=False in ansible.cfg.", "Pseudo-terminal will not be allocated because stdin is not a terminal.", "ssh: Could not resolve hostname cd ~/hdsearch-multinode/profiler/; sudo ~/hdsearch-multinode/profiler/profiler.sh run_profiler 0 : Name or service not known"], "stdout": "/users/cseas002/HDSearch-Multinode/hosts\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nnode1\n8988\nUsing /etc/ansible/ansible.cfg as config file\n\nPLAY [Run remote profiler] *****************************************************\n\nTASK [Gathering Facts] *********************************************************\nok: [node2]\n\nTASK [Run remote profiler] *****************************************************\nchanged: [node2] => {\"ansible_job_id\": \"325883915643.9466\", \"changed\": true, \"finished\": 0, \"results_file\": \"/root/.ansible_async/325883915643.9466\", \"started\": 1}\n\nTASK [Check remote profiler] ***************************************************\nok: [node2] => {\"ansible_job_id\": \"325883915643.9466\", \"changed\": false, \"finished\": 0, \"started\": 1}\n\nPLAY [Kill remote profiler] ****************************************************\n\nPLAY RECAP *********************************************************************\nnode2                      : ok=3    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \n\nnode2\nnode1\nUsing /etc/ansible/ansible.cfg as config file\n\nPLAY [Run remote profiler] *****************************************************\n\nTASK [Gathering Facts] *********************************************************\nok: [node1]\n\nTASK [Run remote profiler] *****************************************************\nchanged: [node1] => {\"ansible_job_id\": \"765713383098.10169\", \"changed\": true, \"finished\": 0, \"results_file\": \"/root/.ansible_async/765713383098.10169\", \"started\": 1}\n\nTASK [Check remote profiler] ***************************************************\nok: [node1] => {\"ansible_job_id\": \"765713383098.10169\", \"changed\": false, \"finished\": 0, \"started\": 1}\n\nPLAY [Kill remote profiler] ****************************************************\n\nPLAY RECAP *********************************************************************\nnode1                      : ok=3    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \n\n\nnode2", "stdout_lines": ["/users/cseas002/HDSearch-Multinode/hosts", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "node1", "8988", "Using /etc/ansible/ansible.cfg as config file", "", "PLAY [Run remote profiler] *****************************************************", "", "TASK [Gathering Facts] *********************************************************", "ok: [node2]", "", "TASK [Run remote profiler] *****************************************************", "changed: [node2] => {\"ansible_job_id\": \"325883915643.9466\", \"changed\": true, \"finished\": 0, \"results_file\": \"/root/.ansible_async/325883915643.9466\", \"started\": 1}", "", "TASK [Check remote profiler] ***************************************************", "ok: [node2] => {\"ansible_job_id\": \"325883915643.9466\", \"changed\": false, \"finished\": 0, \"started\": 1}", "", "PLAY [Kill remote profiler] ****************************************************", "", "PLAY RECAP *********************************************************************", "node2                      : ok=3    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   ", "", "node2", "node1", "Using /etc/ansible/ansible.cfg as config file", "", "PLAY [Run remote profiler] *****************************************************", "", "TASK [Gathering Facts] *********************************************************", "ok: [node1]", "", "TASK [Run remote profiler] *****************************************************", "changed: [node1] => {\"ansible_job_id\": \"765713383098.10169\", \"changed\": true, \"finished\": 0, \"results_file\": \"/root/.ansible_async/765713383098.10169\", \"started\": 1}", "", "TASK [Check remote profiler] ***************************************************", "ok: [node1] => {\"ansible_job_id\": \"765713383098.10169\", \"changed\": false, \"finished\": 0, \"started\": 1}", "", "PLAY [Kill remote profiler] ****************************************************", "", "PLAY RECAP *********************************************************************", "node1                      : ok=3    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   ", "", "", "node2"]}

PLAY [Kill remote profiler] ****************************************************

PLAY RECAP *********************************************************************
node0                      : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

Using /users/cseas002/HDSearch-Multinode/ansible.cfg as config file
[WARNING]: While constructing a mapping from /users/cseas002/HDSearch-
Multinode/ansible/profiler.yml, line 20, column 3, found a duplicate dict key
(hosts). Using last defined value only.
[WARNING]: While constructing a mapping from /users/cseas002/HDSearch-
Multinode/ansible/profiler.yml, line 20, column 3, found a duplicate dict key
(tags). Using last defined value only.
[WARNING]: While constructing a mapping from /users/cseas002/HDSearch-
Multinode/ansible/profiler.yml, line 20, column 3, found a duplicate dict key
(tasks). Using last defined value only.

PLAY [Set profiler] ************************************************************

PLAY [Run remote profiler] *****************************************************

PLAY [Kill remote profiler] ****************************************************

TASK [Gathering Facts] *********************************************************
ok: [node1]
ok: [node2]

TASK [Run remote socwatch] *****************************************************
changed: [node2] => {"ansible_job_id": "652002924770.9671", "changed": true, "finished": 0, "results_file": "/users/cseas002/.ansible_async/652002924770.9671", "started": 1}
changed: [node1] => {"ansible_job_id": "383650520336.10374", "changed": true, "finished": 0, "results_file": "/users/cseas002/.ansible_async/383650520336.10374", "started": 1}

PLAY RECAP *********************************************************************
node1                      : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
node2                      : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

Using /users/cseas002/HDSearch-Multinode/ansible.cfg as config file

PLAY [Initialize cluster manager] **********************************************

PLAY [Initialize workers] ******************************************************

PLAY [Check Status of run] *****************************************************

TASK [Gathering Facts] *********************************************************
ok: [node0]

TASK [Check Status] ************************************************************
changed: [node0] => {"changed": true, "cmd": "~/HDSearch-Multinode/scripts/check-run-status.sh", "delta": "0:01:41.424836", "end": "2023-10-10 14:25:39.123835", "rc": 0, "start": "2023-10-10 14:23:57.698999", "stderr": "", "stderr_lines": [], "stdout": "", "stdout_lines": []}

PLAY [Leave Swarm] *************************************************************

PLAY RECAP *********************************************************************
node0                      : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

INFO:root:./scripts/check-socwatch-status.sh node1
INFO:root:2
INFO:root:./scripts/check-socwatch-status.sh node2
INFO:root:2
INFO:root:sudo docker service logs microsuite_client --raw
INFO:root:client start
INFO:root:mv: cannot stat 'image_feature_vectors.dat': No such file or directory
INFO:root:rm -f *.o *.pb.cc *.pb.h load_generator_open_loop load_generator_closed_loop kill_index_server_empty
INFO:root:g++ -std=c++11 -O3 -mavx2 -mavx -fopenmp  -DMKL_ILP64 -m64 -I/opt/intel/mkl/include -I../ -I/usr/local/include -pthread -O3 -I../ -g -Wall -fopenmp -I../  -c -o helper_files/mid_tier_client_helper.o helper_files/mid_tier_client_helper.cc
INFO:root:[01m[Khelper_files/mid_tier_client_helper.cc:[m[K In function '[01m[Kvoid CreateDatasetFromBinaryFile(const string&, MultiplePoints*)[m[K':
INFO:root:[01m[Khelper_files/mid_tier_client_helper.cc:296:77:[m[K [01;35m[Kwarning: [m[Kcomparison between signed and unsigned integer expressions [[01;35m[K-Wsign-compare[m[K]
INFO:root:         if([01;35m[Kfread(values, sizeof(float), dataset_dimensions, dataset_binary) == dataset_dimensions[m[K)
INFO:root:            [01;35m[K~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~[m[K
INFO:root:[01m[Khelper_files/mid_tier_client_helper.cc:[m[K In function '[01m[Kvoid ResetMetaStats(GlobalStats*, int)[m[K':
INFO:root:[01m[Khelper_files/mid_tier_client_helper.cc:586:31:[m[K [01;35m[Kwarning: [m[Kcomparison between signed and unsigned integer expressions [[01;35m[K-Wsign-compare[m[K]
INFO:root:     for(unsigned int i = 0; [01;35m[Ki < number_of_bucket_servers[m[K; i++)
INFO:root:                             [01;35m[K~~^~~~~~~~~~~~~~~~~~~~~~~~~~[m[K
INFO:root:g++ -std=c++11 -O3 -mavx2 -mavx -fopenmp  -DMKL_ILP64 -m64 -I/opt/intel/mkl/include -I../ -I/usr/local/include -pthread -O3 -I../ -g -Wall -fopenmp -I../  -c -o load_generator_open_loop.o load_generator_open_loop.cc
INFO:root:[01m[Kload_generator_open_loop.cc:[m[K In function '[01m[Kint main(int, char**)[m[K':
INFO:root:[01m[Kload_generator_open_loop.cc:305:51:[m[K [01;35m[Kwarning: [m[Kcomparison between signed and unsigned integer expressions [[01;35m[K-Wsign-compare[m[K]
INFO:root:     while ([01;35m[Kresponses_recvd->AtomicallyReadCount() < overall_queries[m[K)
INFO:root:            [01;35m[K~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~[m[K
INFO:root:[01m[Kload_generator_open_loop.cc:307:75:[m[K [01;35m[Kwarning: [m[Kcomparison between signed and unsigned integer expressions [[01;35m[K-Wsign-compare[m[K]
INFO:root:         if (curr_time >= next_time && [01;35m[Knum_requests->AtomicallyReadCount() < overall_queries[m[K)
INFO:root:                                       [01;35m[K~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~[m[K
INFO:root:g++ ../protoc_files/mid_tier.pb.o ../protoc_files/mid_tier.grpc.pb.o ../bucket_service/src/multiple_points.o ../bucket_service/src/point.o ../bucket_service/src/utils.o ../bucket_service/src/dist_calc.o ../bucket_service/src/custom_priority_queue.o helper_files/mid_tier_client_helper.o helper_files/timing.o helper_files/utils.o load_generator_open_loop.o -O3 -o load_generator_open_loop -L/usr/local/lib `pkg-config --libs grpc++ grpc` -lprotobuf -lpthread -I /usr/local/include -lflann -fopenmp -L/usr/lib64 -lstdc++ -lssl -lcrypto -fopenmp -Wl,--start-group /opt/intel/mkl/lib/intel64/libmkl_intel_ilp64.a /opt/intel/mkl/lib/intel64/libmkl_gnu_thread.a /opt/intel/mkl/lib/intel64/libmkl_core.a -Wl,--end-group -lgomp -lpthread -lm -ldl -I../
INFO:root:g++ -std=c++11 -O3 -mavx2 -mavx -fopenmp  -DMKL_ILP64 -m64 -I/opt/intel/mkl/include -I../ -I/usr/local/include -pthread -O3 -I../ -g -Wall -fopenmp -I../  -c -o load_generator_closed_loop.o load_generator_closed_loop.cc
INFO:root:g++ ../protoc_files/mid_tier.pb.o ../protoc_files/mid_tier.grpc.pb.o ../bucket_service/src/multiple_points.o ../bucket_service/src/point.o ../bucket_service/src/utils.o ../bucket_service/src/dist_calc.o ../bucket_service/src/custom_priority_queue.o helper_files/mid_tier_client_helper.o helper_files/timing.o helper_files/utils.o load_generator_closed_loop.o -O3 -o load_generator_closed_loop -L/usr/local/lib `pkg-config --libs grpc++ grpc` -lprotobuf -lpthread -I /usr/local/include -lflann -fopenmp -L/usr/lib64 -lstdc++ -lssl -lcrypto -fopenmp -Wl,--start-group /opt/intel/mkl/lib/intel64/libmkl_intel_ilp64.a /opt/intel/mkl/lib/intel64/libmkl_gnu_thread.a /opt/intel/mkl/lib/intel64/libmkl_core.a -Wl,--end-group -lgomp -lpthread -lm -ldl -I../
INFO:root:g++ -std=c++11 -O3 -mavx2 -mavx -fopenmp  -DMKL_ILP64 -m64 -I/opt/intel/mkl/include -I../ -I/usr/local/include -pthread -O3 -I../ -g -Wall -fopenmp -I../  -c -o kill_index_server_empty.o kill_index_server_empty.cc
INFO:root:g++ ../protoc_files/mid_tier.pb.o ../protoc_files/mid_tier.grpc.pb.o ../bucket_service/src/multiple_points.o ../bucket_service/src/point.o ../bucket_service/src/utils.o ../bucket_service/src/dist_calc.o ../bucket_service/src/custom_priority_queue.o helper_files/mid_tier_client_helper.o helper_files/timing.o helper_files/utils.o kill_index_server_empty.o -O3 -o kill_index_server_empty -L/usr/local/lib `pkg-config --libs grpc++ grpc` -lprotobuf -lpthread -I /usr/local/include -lflann -fopenmp -L/usr/lib64 -lstdc++ -lssl -lcrypto -fopenmp -Wl,--start-group /opt/intel/mkl/lib/intel64/libmkl_intel_ilp64.a /opt/intel/mkl/lib/intel64/libmkl_gnu_thread.a /opt/intel/mkl/lib/intel64/libmkl_core.a -Wl,--end-group -lgomp -lpthread -lm -ldl -I../
INFO:root:midtier [10.0.1.5] 50054 (?) : Connection refused
INFO:root:midtier [10.0.1.5] 50054 (?) : Connection refused
INFO:root:         @    @       ï¿½           ?                midtier launched
INFO:root:profilers launched
INFO:root:mkdir: cannot create directory './results': File exists
INFO:root:1 1 1 # Requests: 0 
INFO:root:# Responses: 0 
INFO:root:!!!!!!!End of Warmup Period!!!!!!! 
INFO:root:Next time = 1696965820080858.000000
INFO:root:1 1 1 End of Actual Run
INFO:root:# Requests: 119 
INFO:root:# Responses: 119 
INFO:root:Achieved QPS: 0.991667 
INFO:root:Average Response Time(ms): 2.92948 
INFO:root:1.658 1.721 1.792 1.832 1.907 2.026 2.118 5.622 6.873 7.06 11.724 12.588 1.658 1.721 1.792 1.832 1.907 2.026 2.118 5.622 6.873 7.06 11.724 12.588 
INFO:root: Total response time 
INFO:root:1.658 1.721 1.792 1.832 1.907 2.026 2.118 5.622 6.873 7.06 11.724 12.588 
INFO:root:
INFO:root: Index creation time 0 0 0 0 0 0 0 0 0 0 0 0 
INFO:root: Update index util time 0 0 0 0 0 0 0 0.505 0.531 0.549 0.601 0.603 
INFO:root: Unpack loadgen request time 0.053 0.054 0.056 0.057 0.057 0.058 0.058 0.059 0.061 0.063 0.064 0.065 
INFO:root: Get point ids time 
INFO:root:0.116 0.144 0.153 0.179 0.196 0.225 0.308 0.36 0.396 0.442 0.469 0.49 
INFO:root: Total time taken by index server: 
INFO:root:0.58 0.609 0.631 0.684 0.733 0.824 0.873 1.075 1.221 1.4 1.507 1.894 Average Index Time(ms): 0.828286 
INFO:root:
INFO:root: Get bucket responses time 
INFO:root:0.721 0.743 0.768 0.815 0.844 0.894 0.993 4.13 5.129 5.401 10.081 10.992 Average Bucket Time(ms): 1.76501 
INFO:root:
INFO:root: Create bucket request time 0 0 0 0 0 0 0 0 0 0 0 0 
INFO:root: Unpack bucket request time 0.007 0.008 0.018 0.022 0.023 0.023 0.024 0.025 0.027 0.028 0.033 0.065 
INFO:root: Calculate knn time 
INFO:root:0.041 0.042 0.042 0.043 0.044 0.048 0.066 0.108 0.142 0.172 0.178 0.196 
INFO:root: Pack bucket response time 0.001 0.001 0.001 0.001 0.001 0.001 0.002 0.002 0.002 0.002 0.002 0.002 
INFO:root: Unpack bucket response time 0 0 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.002 0.002 
INFO:root: Merge time 0.029 0.03 0.031 0.031 0.034 0.042 0.046 0.047 0.052 0.059 0.07 0.074 
INFO:root: Pack index response time 0.029 0.03 0.031 0.031 0.034 0.042 0.046 0.047 0.052 0.059 0.07 0.074 
INFO:root: Unpack index response time 0.003 0.003 0.003 0.003 0.003 0.004 0.004 0.004 0.004 0.005 0.008 0.008 
INFO:root: 0
INFO:root:sudo touch /users/cseas002/data/TEST5/turbo=False-kernelconfig=baseline-baseline-hyperthreading=False-qps=1-0/hdsearch_client
INFO:root:touch: cannot touch '/users/cseas002/data/TEST5/turbo=False-kernelconfig=baseline-baseline-hyperthreading=False-qps=1-0/hdsearch_client': No such file or directory
INFO:root:sudo chmod 777 /users/cseas002/data/TEST5/turbo=False-kernelconfig=baseline-baseline-hyperthreading=False-qps=1-0/hdsearch_client
INFO:root:chmod: cannot access '/users/cseas002/data/TEST5/turbo=False-kernelconfig=baseline-baseline-hyperthreading=False-qps=1-0/hdsearch_client': No such file or directory
ansible-playbook -v -i hosts -e "" --tags "setup_docker" ansible/install.yml
ansible-playbook -v -i hosts -e "" --tags "set_profiler_hosts" ansible/profiler.yml
ansible-playbook -v -i hosts -e "" --tags "leave_swarm" ansible/hdsearch.yml
ansible-playbook -v -i hosts -e "" --tags "init_master" ansible/hdsearch.yml
ansible-playbook -v -i hosts -e "" --tags "init_workers" ansible/hdsearch.yml
ansible-playbook -v -i hosts -e "MSR_VALUE=0x1414"  ansible/configure.yml
ansible-playbook -v -i hosts -e "" --tags "kill_profiler" ansible/profiler.yml
ansible-playbook -v -i hosts -e "HOST_FILE=/users/cseas002/HDSearch-Multinode/hosts ITERATION=0" --tags "run_profiler" ansible/profiler.yml
Profilerrrrr putput 0
ansible-playbook -v -i hosts -e "MONITOR_TIME=40 OUTPUT_FILE=turbo=False-kernelconfig=baseline-baseline-hyperthreading=False-qps=1" --tags "run_socwatch" ./ansible/profiler.yml
ansible-playbook -v -i hosts -e "" --tags "check_status" ansible/hdsearch.yml
Traceback (most recent call last):
  File "run_experiment.py", line 444, in <module>
    main(sys.argv[1:])
  File "run_experiment.py", line 440, in main
    run_multiple_experiments('/users/cseas002/data', batch_name, system_conf, client_conf, midtier_conf, bucket_conf, iter)
  File "run_experiment.py", line 365, in run_multiple_experiments
    status = run_single_experiment(system_conf,root_results_dir, name_prefix, instance_conf, iter,bucket,midtier)
  File "run_experiment.py", line 323, in run_single_experiment
    with open(client_results_path_name, 'w') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '/users/cseas002/data/TEST5/turbo=False-kernelconfig=baseline-baseline-hyperthreading=False-qps=1-0/hdsearch_client'
