Using /users/cseas002/HDSearch-Multinode/ansible.cfg as config file

PLAY [Install HDSearch] ********************************************************

TASK [Gathering Facts] *********************************************************
ok: [node2]
ok: [node1]

PLAY [Set Up Docker Curl] ******************************************************

TASK [Gathering Facts] *********************************************************
ok: [node2]
ok: [node1]
ok: [node0]

PLAY [Make space to commit] ****************************************************

TASK [Gathering Facts] *********************************************************
ok: [node1]
ok: [node2]
ok: [node0]

TASK [Make Space to Commit Image] **********************************************
[WARNING]: Consider using 'become', 'become_method', and 'become_user' rather
than running sudo
changed: [node1] => {"changed": true, "cmd": ["sudo", "~/HDSearch-Multinode/scripts/change-storage-location-docker.sh"], "delta": "0:00:07.447097", "end": "2023-09-18 14:50:25.569997", "rc": 0, "start": "2023-09-18 14:50:18.122900", "stderr": "Warning: Stopping docker.service, but it can still be activated by:\n  docker.socket\nmkdir: cannot create directory ‘/dev/mkdocker’: File exists", "stderr_lines": ["Warning: Stopping docker.service, but it can still be activated by:", "  docker.socket", "mkdir: cannot create directory ‘/dev/mkdocker’: File exists"], "stdout": "77c75943cb10\nUntagged: mklean/baseline-microsuite@sha256:d01aee25d7aebe6624aa5b1acccf75d5554700f866a7fdc57676aec74a86c44c\nDeleted: sha256:90f8ba6af25a3e89eccd56ed7453e1c91120b13f25c92f72956548f552acdb15\nDeleted: sha256:7d7bb52e2e991a9ef12f46b9ca755d11ab136da63216c3d7b3ae7e5db1ac8750\nDeleted: sha256:a8a53f5f6c8630b73641635133a526fc81a347a1b736bd558e97463a7d290314\nDeleted: sha256:9bf3932968c1bb680c8217ba99ee2d0179f5e308673f184da4e36e3c20709144\nDeleted: sha256:750af7b8c4edd1bde0d80babfc97b10ff9be844dade2e16524654329c3e3acf1\nDeleted: sha256:aa99de0dee7cececefd190f4be8b847d4b14198bcdeb72f7b5a1fb7d828b58a1\nDeleted: sha256:822e0342cb2ed6b3ede926b31766466892d8c3c014017d8223f42b1eb8730098\nDeleted: sha256:9bd80ab5dfc88af200c32bb0a6987831e3daa06505699cd4793eaa318c48b0c4\nDeleted: sha256:f00fab48208aa68c0baf2ab85900f7e82bdd99b2cd823dcc80d698fe3a3fa80d\nDeleted: sha256:16f559e8ec664cf176d3f2533a13bf78862e73e6e0e4ee076545495335c74b55\nDeleted: sha256:252cd7005ec1a73c4bca3c05f0a09abc9374b0b2fdc023562f99ddada31744e8\nDeleted: sha256:792c379c86dbd2250a7429bc78ef64b9524d9c49c163fa5398092505967ed230\nDeleted: sha256:e7ed01c68431b65f90c923142649429a795ab660e23ea6af15d522ccad753b59\nDeleted: sha256:e722d396f503c712107acad2a081b07e33e73d6286c43f58234f69345a216918", "stdout_lines": ["77c75943cb10", "Untagged: mklean/baseline-microsuite@sha256:d01aee25d7aebe6624aa5b1acccf75d5554700f866a7fdc57676aec74a86c44c", "Deleted: sha256:90f8ba6af25a3e89eccd56ed7453e1c91120b13f25c92f72956548f552acdb15", "Deleted: sha256:7d7bb52e2e991a9ef12f46b9ca755d11ab136da63216c3d7b3ae7e5db1ac8750", "Deleted: sha256:a8a53f5f6c8630b73641635133a526fc81a347a1b736bd558e97463a7d290314", "Deleted: sha256:9bf3932968c1bb680c8217ba99ee2d0179f5e308673f184da4e36e3c20709144", "Deleted: sha256:750af7b8c4edd1bde0d80babfc97b10ff9be844dade2e16524654329c3e3acf1", "Deleted: sha256:aa99de0dee7cececefd190f4be8b847d4b14198bcdeb72f7b5a1fb7d828b58a1", "Deleted: sha256:822e0342cb2ed6b3ede926b31766466892d8c3c014017d8223f42b1eb8730098", "Deleted: sha256:9bd80ab5dfc88af200c32bb0a6987831e3daa06505699cd4793eaa318c48b0c4", "Deleted: sha256:f00fab48208aa68c0baf2ab85900f7e82bdd99b2cd823dcc80d698fe3a3fa80d", "Deleted: sha256:16f559e8ec664cf176d3f2533a13bf78862e73e6e0e4ee076545495335c74b55", "Deleted: sha256:252cd7005ec1a73c4bca3c05f0a09abc9374b0b2fdc023562f99ddada31744e8", "Deleted: sha256:792c379c86dbd2250a7429bc78ef64b9524d9c49c163fa5398092505967ed230", "Deleted: sha256:e7ed01c68431b65f90c923142649429a795ab660e23ea6af15d522ccad753b59", "Deleted: sha256:e722d396f503c712107acad2a081b07e33e73d6286c43f58234f69345a216918"]}
changed: [node0] => {"changed": true, "cmd": ["sudo", "~/HDSearch-Multinode/scripts/change-storage-location-docker.sh"], "delta": "0:00:08.174719", "end": "2023-09-18 14:50:26.297552", "rc": 0, "start": "2023-09-18 14:50:18.122833", "stderr": "Warning: Stopping docker.service, but it can still be activated by:\n  docker.socket\nmkdir: cannot create directory ‘/dev/mkdocker’: File exists", "stderr_lines": ["Warning: Stopping docker.service, but it can still be activated by:", "  docker.socket", "mkdir: cannot create directory ‘/dev/mkdocker’: File exists"], "stdout": "9f85153fccf6\nUntagged: mklean/baseline-microsuite@sha256:d01aee25d7aebe6624aa5b1acccf75d5554700f866a7fdc57676aec74a86c44c\nDeleted: sha256:90f8ba6af25a3e89eccd56ed7453e1c91120b13f25c92f72956548f552acdb15\nDeleted: sha256:7d7bb52e2e991a9ef12f46b9ca755d11ab136da63216c3d7b3ae7e5db1ac8750\nDeleted: sha256:a8a53f5f6c8630b73641635133a526fc81a347a1b736bd558e97463a7d290314\nDeleted: sha256:9bf3932968c1bb680c8217ba99ee2d0179f5e308673f184da4e36e3c20709144\nDeleted: sha256:750af7b8c4edd1bde0d80babfc97b10ff9be844dade2e16524654329c3e3acf1\nDeleted: sha256:aa99de0dee7cececefd190f4be8b847d4b14198bcdeb72f7b5a1fb7d828b58a1\nDeleted: sha256:822e0342cb2ed6b3ede926b31766466892d8c3c014017d8223f42b1eb8730098\nDeleted: sha256:9bd80ab5dfc88af200c32bb0a6987831e3daa06505699cd4793eaa318c48b0c4\nDeleted: sha256:f00fab48208aa68c0baf2ab85900f7e82bdd99b2cd823dcc80d698fe3a3fa80d\nDeleted: sha256:16f559e8ec664cf176d3f2533a13bf78862e73e6e0e4ee076545495335c74b55\nDeleted: sha256:252cd7005ec1a73c4bca3c05f0a09abc9374b0b2fdc023562f99ddada31744e8\nDeleted: sha256:792c379c86dbd2250a7429bc78ef64b9524d9c49c163fa5398092505967ed230\nDeleted: sha256:e7ed01c68431b65f90c923142649429a795ab660e23ea6af15d522ccad753b59\nDeleted: sha256:e722d396f503c712107acad2a081b07e33e73d6286c43f58234f69345a216918", "stdout_lines": ["9f85153fccf6", "Untagged: mklean/baseline-microsuite@sha256:d01aee25d7aebe6624aa5b1acccf75d5554700f866a7fdc57676aec74a86c44c", "Deleted: sha256:90f8ba6af25a3e89eccd56ed7453e1c91120b13f25c92f72956548f552acdb15", "Deleted: sha256:7d7bb52e2e991a9ef12f46b9ca755d11ab136da63216c3d7b3ae7e5db1ac8750", "Deleted: sha256:a8a53f5f6c8630b73641635133a526fc81a347a1b736bd558e97463a7d290314", "Deleted: sha256:9bf3932968c1bb680c8217ba99ee2d0179f5e308673f184da4e36e3c20709144", "Deleted: sha256:750af7b8c4edd1bde0d80babfc97b10ff9be844dade2e16524654329c3e3acf1", "Deleted: sha256:aa99de0dee7cececefd190f4be8b847d4b14198bcdeb72f7b5a1fb7d828b58a1", "Deleted: sha256:822e0342cb2ed6b3ede926b31766466892d8c3c014017d8223f42b1eb8730098", "Deleted: sha256:9bd80ab5dfc88af200c32bb0a6987831e3daa06505699cd4793eaa318c48b0c4", "Deleted: sha256:f00fab48208aa68c0baf2ab85900f7e82bdd99b2cd823dcc80d698fe3a3fa80d", "Deleted: sha256:16f559e8ec664cf176d3f2533a13bf78862e73e6e0e4ee076545495335c74b55", "Deleted: sha256:252cd7005ec1a73c4bca3c05f0a09abc9374b0b2fdc023562f99ddada31744e8", "Deleted: sha256:792c379c86dbd2250a7429bc78ef64b9524d9c49c163fa5398092505967ed230", "Deleted: sha256:e7ed01c68431b65f90c923142649429a795ab660e23ea6af15d522ccad753b59", "Deleted: sha256:e722d396f503c712107acad2a081b07e33e73d6286c43f58234f69345a216918"]}
changed: [node2] => {"changed": true, "cmd": ["sudo", "~/HDSearch-Multinode/scripts/change-storage-location-docker.sh"], "delta": "0:00:10.664813", "end": "2023-09-18 14:50:28.785013", "rc": 0, "start": "2023-09-18 14:50:18.120200", "stderr": "Warning: Stopping docker.service, but it can still be activated by:\n  docker.socket\nmkdir: cannot create directory ‘/dev/mkdocker’: File exists", "stderr_lines": ["Warning: Stopping docker.service, but it can still be activated by:", "  docker.socket", "mkdir: cannot create directory ‘/dev/mkdocker’: File exists"], "stdout": "b396d860bc18\nUntagged: mklean/baseline-microsuite@sha256:d01aee25d7aebe6624aa5b1acccf75d5554700f866a7fdc57676aec74a86c44c\nDeleted: sha256:90f8ba6af25a3e89eccd56ed7453e1c91120b13f25c92f72956548f552acdb15\nDeleted: sha256:7d7bb52e2e991a9ef12f46b9ca755d11ab136da63216c3d7b3ae7e5db1ac8750\nDeleted: sha256:a8a53f5f6c8630b73641635133a526fc81a347a1b736bd558e97463a7d290314\nDeleted: sha256:9bf3932968c1bb680c8217ba99ee2d0179f5e308673f184da4e36e3c20709144\nDeleted: sha256:750af7b8c4edd1bde0d80babfc97b10ff9be844dade2e16524654329c3e3acf1\nDeleted: sha256:aa99de0dee7cececefd190f4be8b847d4b14198bcdeb72f7b5a1fb7d828b58a1\nDeleted: sha256:822e0342cb2ed6b3ede926b31766466892d8c3c014017d8223f42b1eb8730098\nDeleted: sha256:9bd80ab5dfc88af200c32bb0a6987831e3daa06505699cd4793eaa318c48b0c4\nDeleted: sha256:f00fab48208aa68c0baf2ab85900f7e82bdd99b2cd823dcc80d698fe3a3fa80d\nDeleted: sha256:16f559e8ec664cf176d3f2533a13bf78862e73e6e0e4ee076545495335c74b55\nDeleted: sha256:252cd7005ec1a73c4bca3c05f0a09abc9374b0b2fdc023562f99ddada31744e8\nDeleted: sha256:792c379c86dbd2250a7429bc78ef64b9524d9c49c163fa5398092505967ed230\nDeleted: sha256:e7ed01c68431b65f90c923142649429a795ab660e23ea6af15d522ccad753b59\nDeleted: sha256:e722d396f503c712107acad2a081b07e33e73d6286c43f58234f69345a216918", "stdout_lines": ["b396d860bc18", "Untagged: mklean/baseline-microsuite@sha256:d01aee25d7aebe6624aa5b1acccf75d5554700f866a7fdc57676aec74a86c44c", "Deleted: sha256:90f8ba6af25a3e89eccd56ed7453e1c91120b13f25c92f72956548f552acdb15", "Deleted: sha256:7d7bb52e2e991a9ef12f46b9ca755d11ab136da63216c3d7b3ae7e5db1ac8750", "Deleted: sha256:a8a53f5f6c8630b73641635133a526fc81a347a1b736bd558e97463a7d290314", "Deleted: sha256:9bf3932968c1bb680c8217ba99ee2d0179f5e308673f184da4e36e3c20709144", "Deleted: sha256:750af7b8c4edd1bde0d80babfc97b10ff9be844dade2e16524654329c3e3acf1", "Deleted: sha256:aa99de0dee7cececefd190f4be8b847d4b14198bcdeb72f7b5a1fb7d828b58a1", "Deleted: sha256:822e0342cb2ed6b3ede926b31766466892d8c3c014017d8223f42b1eb8730098", "Deleted: sha256:9bd80ab5dfc88af200c32bb0a6987831e3daa06505699cd4793eaa318c48b0c4", "Deleted: sha256:f00fab48208aa68c0baf2ab85900f7e82bdd99b2cd823dcc80d698fe3a3fa80d", "Deleted: sha256:16f559e8ec664cf176d3f2533a13bf78862e73e6e0e4ee076545495335c74b55", "Deleted: sha256:252cd7005ec1a73c4bca3c05f0a09abc9374b0b2fdc023562f99ddada31744e8", "Deleted: sha256:792c379c86dbd2250a7429bc78ef64b9524d9c49c163fa5398092505967ed230", "Deleted: sha256:e7ed01c68431b65f90c923142649429a795ab660e23ea6af15d522ccad753b59", "Deleted: sha256:e722d396f503c712107acad2a081b07e33e73d6286c43f58234f69345a216918"]}

PLAY [Install Profiler Dep] ****************************************************

PLAY RECAP *********************************************************************
node0                      : ok=3    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
node1                      : ok=4    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
node2                      : ok=4    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

Using /users/cseas002/HDSearch-Multinode/ansible.cfg as config file
[WARNING]: While constructing a mapping from /users/cseas002/HDSearch-
Multinode/ansible/profiler.yml, line 20, column 3, found a duplicate dict key
(hosts). Using last defined value only.
[WARNING]: While constructing a mapping from /users/cseas002/HDSearch-
Multinode/ansible/profiler.yml, line 20, column 3, found a duplicate dict key
(tags). Using last defined value only.
[WARNING]: While constructing a mapping from /users/cseas002/HDSearch-
Multinode/ansible/profiler.yml, line 20, column 3, found a duplicate dict key
(tasks). Using last defined value only.

PLAY [Set profiler] ************************************************************

TASK [Gathering Facts] *********************************************************
ok: [node1]
ok: [node2]

TASK [Set profiler Hosts] ******************************************************
changed: [node1] => {"ansible_job_id": "687934939176.20228", "changed": true, "finished": 0, "results_file": "/users/cseas002/.ansible_async/687934939176.20228", "started": 1}
changed: [node2] => {"ansible_job_id": "51946758258.10398", "changed": true, "finished": 0, "results_file": "/users/cseas002/.ansible_async/51946758258.10398", "started": 1}

PLAY [Run remote profiler] *****************************************************

PLAY [Kill remote profiler] ****************************************************

PLAY RECAP *********************************************************************
node1                      : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
node2                      : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

Using /users/cseas002/HDSearch-Multinode/ansible.cfg as config file

PLAY [Initialize cluster manager] **********************************************

PLAY [Initialize workers] ******************************************************

PLAY [Check Status of run] *****************************************************

PLAY [Leave Swarm] *************************************************************

TASK [Gathering Facts] *********************************************************
ok: [node1]
ok: [node0]
ok: [node2]

TASK [Leave Swarm] *************************************************************
changed: [node2] => {"ansible_job_id": "327564155411.10521", "changed": true, "finished": 0, "results_file": "/users/cseas002/.ansible_async/327564155411.10521", "started": 1}
changed: [node1] => {"ansible_job_id": "484380079233.20345", "changed": true, "finished": 0, "results_file": "/users/cseas002/.ansible_async/484380079233.20345", "started": 1}
changed: [node0] => {"ansible_job_id": "473085223290.14098", "changed": true, "finished": 0, "results_file": "/users/cseas002/.ansible_async/473085223290.14098", "started": 1}

PLAY RECAP *********************************************************************
node0                      : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
node1                      : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
node2                      : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

Using /users/cseas002/HDSearch-Multinode/ansible.cfg as config file

PLAY [Initialize cluster manager] **********************************************

TASK [Gathering Facts] *********************************************************
ok: [node0]

TASK [Init Master] *************************************************************
[WARNING]: Consider using 'become', 'become_method', and 'become_user' rather
than running sudo
changed: [node0] => {"changed": true, "cmd": "sudo docker swarm init --advertise-addr 10.10.1.1", "delta": "0:00:00.605955", "end": "2023-09-18 14:50:38.480184", "rc": 0, "start": "2023-09-18 14:50:37.874229", "stderr": "", "stderr_lines": [], "stdout": "Swarm initialized: current node (b07fv0wvouabfu8rcwoen7e04) is now a manager.\n\nTo add a worker to this swarm, run the following command:\n\n    docker swarm join --token SWMTKN-1-2yn6mwzw703jyu7x9k30i05hofi7qve3h1nm6lmk0ihypse33z-5a64x40fiss3lti9aito873o9 10.10.1.1:2377\n\nTo add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.", "stdout_lines": ["Swarm initialized: current node (b07fv0wvouabfu8rcwoen7e04) is now a manager.", "", "To add a worker to this swarm, run the following command:", "", "    docker swarm join --token SWMTKN-1-2yn6mwzw703jyu7x9k30i05hofi7qve3h1nm6lmk0ihypse33z-5a64x40fiss3lti9aito873o9 10.10.1.1:2377", "", "To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions."]}

PLAY [Initialize workers] ******************************************************

PLAY [Check Status of run] *****************************************************

PLAY [Leave Swarm] *************************************************************

PLAY RECAP *********************************************************************
node0                      : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

Using /users/cseas002/HDSearch-Multinode/ansible.cfg as config file

PLAY [Initialize cluster manager] **********************************************

PLAY [Initialize workers] ******************************************************

TASK [Gathering Facts] *********************************************************
ok: [node1]
ok: [node2]

TASK [Init Workers] ************************************************************
changed: [node1] => {"ansible_job_id": "160730111989.20501", "changed": true, "finished": 0, "results_file": "/users/cseas002/.ansible_async/160730111989.20501", "started": 1}
changed: [node2] => {"ansible_job_id": "654182860201.10681", "changed": true, "finished": 0, "results_file": "/users/cseas002/.ansible_async/654182860201.10681", "started": 1}

PLAY [Check Status of run] *****************************************************

PLAY [Leave Swarm] *************************************************************

PLAY RECAP *********************************************************************
node1                      : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
node2                      : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

Using /users/cseas002/HDSearch-Multinode/ansible.cfg as config file

PLAY [Set uncore frequency] ****************************************************

TASK [Gathering Facts] *********************************************************
ok: [node2]
ok: [node1]

TASK [Load msr kernel module] **************************************************
changed: [node2] => {"changed": true, "cmd": "modprobe msr", "delta": "0:00:00.004491", "end": "2023-09-18 14:50:44.506431", "rc": 0, "start": "2023-09-18 14:50:44.501940", "stderr": "", "stderr_lines": [], "stdout": "", "stdout_lines": []}
changed: [node1] => {"changed": true, "cmd": "modprobe msr", "delta": "0:00:00.004134", "end": "2023-09-18 14:50:44.512054", "rc": 0, "start": "2023-09-18 14:50:44.507920", "stderr": "", "stderr_lines": [], "stdout": "", "stdout_lines": []}

TASK [Set uncore freq] *********************************************************
changed: [node2] => {"changed": true, "cmd": "wrmsr -p0 0x620 0x1414", "delta": "0:00:00.003845", "end": "2023-09-18 14:50:44.776439", "rc": 0, "start": "2023-09-18 14:50:44.772594", "stderr": "", "stderr_lines": [], "stdout": "", "stdout_lines": []}
changed: [node1] => {"changed": true, "cmd": "wrmsr -p0 0x620 0x1414", "delta": "0:00:00.003646", "end": "2023-09-18 14:50:44.792630", "rc": 0, "start": "2023-09-18 14:50:44.788984", "stderr": "", "stderr_lines": [], "stdout": "", "stdout_lines": []}

PLAY RECAP *********************************************************************
node1                      : ok=3    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
node2                      : ok=3    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

Nothing found in stack: microsuite
Using /users/cseas002/HDSearch-Multinode/ansible.cfg as config file
[WARNING]: While constructing a mapping from /users/cseas002/HDSearch-
Multinode/ansible/profiler.yml, line 20, column 3, found a duplicate dict key
(hosts). Using last defined value only.
[WARNING]: While constructing a mapping from /users/cseas002/HDSearch-
Multinode/ansible/profiler.yml, line 20, column 3, found a duplicate dict key
(tags). Using last defined value only.
[WARNING]: While constructing a mapping from /users/cseas002/HDSearch-
Multinode/ansible/profiler.yml, line 20, column 3, found a duplicate dict key
(tasks). Using last defined value only.

PLAY [Set profiler] ************************************************************

PLAY [Run remote profiler] *****************************************************

PLAY [Kill remote profiler] ****************************************************

PLAY RECAP *********************************************************************

INFO:root:ssh -A node0 hostname
INFO:root:node0.hdsearch2.ramp-pg0.wisc.cloudlab.us
INFO:root:ssh -A node1 hostname
INFO:root:node1.hdsearch2.ramp-pg0.wisc.cloudlab.us
INFO:root:ssh -A node2 hostname
INFO:root:node2.hdsearch2.ramp-pg0.wisc.cloudlab.us
Ignoring deprecated options:

expose: Exposing ports is unnecessary - services on the same network can access each other's containers on any port.

Creating network microsuite_default
Creating service microsuite_bucket
Creating service microsuite_midtier
Creating service microsuite_client
Using /users/cseas002/HDSearch-Multinode/ansible.cfg as config file
[WARNING]: While constructing a mapping from /users/cseas002/HDSearch-
Multinode/ansible/profiler.yml, line 20, column 3, found a duplicate dict key
(hosts). Using last defined value only.
[WARNING]: While constructing a mapping from /users/cseas002/HDSearch-
Multinode/ansible/profiler.yml, line 20, column 3, found a duplicate dict key
(tags). Using last defined value only.
[WARNING]: While constructing a mapping from /users/cseas002/HDSearch-
Multinode/ansible/profiler.yml, line 20, column 3, found a duplicate dict key
(tasks). Using last defined value only.

PLAY [Set profiler] ************************************************************

PLAY [Run remote profiler] *****************************************************

TASK [Gathering Facts] *********************************************************
ok: [node0]

TASK [Run remote profiler] *****************************************************
changed: [node0] => {"changed": true, "cmd": "~/HDSearch-Multinode/scripts/startProfiler.sh ~/HDSearch-Multinode/hosts 0", "delta": "0:15:25.147592", "end": "2023-09-18 15:06:33.670614", "rc": 0, "start": "2023-09-18 14:51:08.523022", "stderr": "[DEPRECATION WARNING]: Distribution Ubuntu 18.04 on host node2 should use \n/usr/bin/python3, but is using /usr/bin/python for backward compatibility with \nprior Ansible releases. A future Ansible release will default to using the \ndiscovered platform python for this host. See https://docs.ansible.com/ansible/\n2.9/reference_appendices/interpreter_discovery.html for more information. This \nfeature will be removed in version 2.12. Deprecation warnings can be disabled \nby setting deprecation_warnings=False in ansible.cfg.\n[DEPRECATION WARNING]: Distribution Ubuntu 18.04 on host node1 should use \n/usr/bin/python3, but is using /usr/bin/python for backward compatibility with \nprior Ansible releases. A future Ansible release will default to using the \ndiscovered platform python for this host. See https://docs.ansible.com/ansible/\n2.9/reference_appendices/interpreter_discovery.html for more information. This \nfeature will be removed in version 2.12. Deprecation warnings can be disabled \nby setting deprecation_warnings=False in ansible.cfg.\nPseudo-terminal will not be allocated because stdin is not a terminal.\r\nssh: Could not resolve hostname cd ~/hdsearch-multinode/profiler/; sudo ~/hdsearch-multinode/profiler/profiler.sh run_profiler 0 : Name or service not known", "stderr_lines": ["[DEPRECATION WARNING]: Distribution Ubuntu 18.04 on host node2 should use ", "/usr/bin/python3, but is using /usr/bin/python for backward compatibility with ", "prior Ansible releases. A future Ansible release will default to using the ", "discovered platform python for this host. See https://docs.ansible.com/ansible/", "2.9/reference_appendices/interpreter_discovery.html for more information. This ", "feature will be removed in version 2.12. Deprecation warnings can be disabled ", "by setting deprecation_warnings=False in ansible.cfg.", "[DEPRECATION WARNING]: Distribution Ubuntu 18.04 on host node1 should use ", "/usr/bin/python3, but is using /usr/bin/python for backward compatibility with ", "prior Ansible releases. A future Ansible release will default to using the ", "discovered platform python for this host. See https://docs.ansible.com/ansible/", "2.9/reference_appendices/interpreter_discovery.html for more information. This ", "feature will be removed in version 2.12. Deprecation warnings can be disabled ", "by setting deprecation_warnings=False in ansible.cfg.", "Pseudo-terminal will not be allocated because stdin is not a terminal.", "ssh: Could not resolve hostname cd ~/hdsearch-multinode/profiler/; sudo ~/hdsearch-multinode/profiler/profiler.sh run_profiler 0 : Name or service not known"], "stdout": "/users/cseas002/HDSearch-Multinode/hosts\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nsleep\nnode1\n11598\nUsing /etc/ansible/ansible.cfg as config file\n\nPLAY [Run remote profiler] *****************************************************\n\nTASK [Gathering Facts] *********************************************************\nok: [node2]\n\nTASK [Run remote profiler] *****************************************************\nchanged: [node2] => {\"ansible_job_id\": \"475186173379.12765\", \"changed\": true, \"finished\": 0, \"results_file\": \"/root/.ansible_async/475186173379.12765\", \"started\": 1}\n\nTASK [Check remote profiler] ***************************************************\nfatal: [node2]: FAILED! => {\"ansible_job_id\": \"475186173379.12765\", \"changed\": true, \"cmd\": [\"python3\", \"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\", \"-i\", \"0\", \"-t\", \"11598\"], \"delta\": \"0:00:00.121932\", \"end\": \"2023-09-18 15:06:29.022472\", \"finished\": 1, \"msg\": \"non-zero return code\", \"rc\": 1, \"start\": \"2023-09-18 15:06:28.900540\", \"stderr\": \"Traceback (most recent call last):\\n  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 563, in <module>\\n    main()\\n  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 554, in main\\n    real_main()\\n  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 551, in real_main\\n    parse_args()\\n  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 548, in parse_args\\n    server(args.port,args.perf_iteration,args.process_id)\\n  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 433, in server\\n    server = SimpleXMLRPCServer((hostname, port), allow_none=True)\\n  File \\\"/usr/lib/python3.6/xmlrpc/server.py\\\", line 599, in __init__\\n    socketserver.TCPServer.__init__(self, addr, requestHandler, bind_and_activate)\\n  File \\\"/usr/lib/python3.6/socketserver.py\\\", line 456, in __init__\\n    self.server_bind()\\n  File \\\"/usr/lib/python3.6/socketserver.py\\\", line 470, in server_bind\\n    self.socket.bind(self.server_address)\\nOSError: [Errno 98] Address already in use\", \"stderr_lines\": [\"Traceback (most recent call last):\", \"  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 563, in <module>\", \"    main()\", \"  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 554, in main\", \"    real_main()\", \"  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 551, in real_main\", \"    parse_args()\", \"  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 548, in parse_args\", \"    server(args.port,args.perf_iteration,args.process_id)\", \"  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 433, in server\", \"    server = SimpleXMLRPCServer((hostname, port), allow_none=True)\", \"  File \\\"/usr/lib/python3.6/xmlrpc/server.py\\\", line 599, in __init__\", \"    socketserver.TCPServer.__init__(self, addr, requestHandler, bind_and_activate)\", \"  File \\\"/usr/lib/python3.6/socketserver.py\\\", line 456, in __init__\", \"    self.server_bind()\", \"  File \\\"/usr/lib/python3.6/socketserver.py\\\", line 470, in server_bind\", \"    self.socket.bind(self.server_address)\", \"OSError: [Errno 98] Address already in use\"], \"stdout\": \"\", \"stdout_lines\": []}\n\nPLAY RECAP *********************************************************************\nnode2                      : ok=2    changed=1    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   \n\nnode2\nnode1\nUsing /etc/ansible/ansible.cfg as config file\n\nPLAY [Run remote profiler] *****************************************************\n\nTASK [Gathering Facts] *********************************************************\nok: [node1]\n\nTASK [Run remote profiler] *****************************************************\nchanged: [node1] => {\"ansible_job_id\": \"780942345903.22141\", \"changed\": true, \"finished\": 0, \"results_file\": \"/root/.ansible_async/780942345903.22141\", \"started\": 1}\n\nTASK [Check remote profiler] ***************************************************\nfatal: [node1]: FAILED! => {\"ansible_job_id\": \"780942345903.22141\", \"changed\": true, \"cmd\": [\"python3\", \"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\", \"-i\", \"0\", \"-t\", \"21154\"], \"delta\": \"0:00:00.121923\", \"end\": \"2023-09-18 15:06:33.021734\", \"finished\": 1, \"msg\": \"non-zero return code\", \"rc\": 1, \"start\": \"2023-09-18 15:06:32.899811\", \"stderr\": \"Traceback (most recent call last):\\n  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 563, in <module>\\n    main()\\n  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 554, in main\\n    real_main()\\n  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 551, in real_main\\n    parse_args()\\n  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 548, in parse_args\\n    server(args.port,args.perf_iteration,args.process_id)\\n  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 433, in server\\n    server = SimpleXMLRPCServer((hostname, port), allow_none=True)\\n  File \\\"/usr/lib/python3.6/xmlrpc/server.py\\\", line 599, in __init__\\n    socketserver.TCPServer.__init__(self, addr, requestHandler, bind_and_activate)\\n  File \\\"/usr/lib/python3.6/socketserver.py\\\", line 456, in __init__\\n    self.server_bind()\\n  File \\\"/usr/lib/python3.6/socketserver.py\\\", line 470, in server_bind\\n    self.socket.bind(self.server_address)\\nOSError: [Errno 98] Address already in use\", \"stderr_lines\": [\"Traceback (most recent call last):\", \"  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 563, in <module>\", \"    main()\", \"  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 554, in main\", \"    real_main()\", \"  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 551, in real_main\", \"    parse_args()\", \"  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 548, in parse_args\", \"    server(args.port,args.perf_iteration,args.process_id)\", \"  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 433, in server\", \"    server = SimpleXMLRPCServer((hostname, port), allow_none=True)\", \"  File \\\"/usr/lib/python3.6/xmlrpc/server.py\\\", line 599, in __init__\", \"    socketserver.TCPServer.__init__(self, addr, requestHandler, bind_and_activate)\", \"  File \\\"/usr/lib/python3.6/socketserver.py\\\", line 456, in __init__\", \"    self.server_bind()\", \"  File \\\"/usr/lib/python3.6/socketserver.py\\\", line 470, in server_bind\", \"    self.socket.bind(self.server_address)\", \"OSError: [Errno 98] Address already in use\"], \"stdout\": \"\", \"stdout_lines\": []}\n\nPLAY RECAP *********************************************************************\nnode1                      : ok=2    changed=1    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   \n\n\nnode2", "stdout_lines": ["/users/cseas002/HDSearch-Multinode/hosts", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "sleep", "node1", "11598", "Using /etc/ansible/ansible.cfg as config file", "", "PLAY [Run remote profiler] *****************************************************", "", "TASK [Gathering Facts] *********************************************************", "ok: [node2]", "", "TASK [Run remote profiler] *****************************************************", "changed: [node2] => {\"ansible_job_id\": \"475186173379.12765\", \"changed\": true, \"finished\": 0, \"results_file\": \"/root/.ansible_async/475186173379.12765\", \"started\": 1}", "", "TASK [Check remote profiler] ***************************************************", "fatal: [node2]: FAILED! => {\"ansible_job_id\": \"475186173379.12765\", \"changed\": true, \"cmd\": [\"python3\", \"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\", \"-i\", \"0\", \"-t\", \"11598\"], \"delta\": \"0:00:00.121932\", \"end\": \"2023-09-18 15:06:29.022472\", \"finished\": 1, \"msg\": \"non-zero return code\", \"rc\": 1, \"start\": \"2023-09-18 15:06:28.900540\", \"stderr\": \"Traceback (most recent call last):\\n  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 563, in <module>\\n    main()\\n  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 554, in main\\n    real_main()\\n  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 551, in real_main\\n    parse_args()\\n  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 548, in parse_args\\n    server(args.port,args.perf_iteration,args.process_id)\\n  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 433, in server\\n    server = SimpleXMLRPCServer((hostname, port), allow_none=True)\\n  File \\\"/usr/lib/python3.6/xmlrpc/server.py\\\", line 599, in __init__\\n    socketserver.TCPServer.__init__(self, addr, requestHandler, bind_and_activate)\\n  File \\\"/usr/lib/python3.6/socketserver.py\\\", line 456, in __init__\\n    self.server_bind()\\n  File \\\"/usr/lib/python3.6/socketserver.py\\\", line 470, in server_bind\\n    self.socket.bind(self.server_address)\\nOSError: [Errno 98] Address already in use\", \"stderr_lines\": [\"Traceback (most recent call last):\", \"  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 563, in <module>\", \"    main()\", \"  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 554, in main\", \"    real_main()\", \"  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 551, in real_main\", \"    parse_args()\", \"  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 548, in parse_args\", \"    server(args.port,args.perf_iteration,args.process_id)\", \"  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 433, in server\", \"    server = SimpleXMLRPCServer((hostname, port), allow_none=True)\", \"  File \\\"/usr/lib/python3.6/xmlrpc/server.py\\\", line 599, in __init__\", \"    socketserver.TCPServer.__init__(self, addr, requestHandler, bind_and_activate)\", \"  File \\\"/usr/lib/python3.6/socketserver.py\\\", line 456, in __init__\", \"    self.server_bind()\", \"  File \\\"/usr/lib/python3.6/socketserver.py\\\", line 470, in server_bind\", \"    self.socket.bind(self.server_address)\", \"OSError: [Errno 98] Address already in use\"], \"stdout\": \"\", \"stdout_lines\": []}", "", "PLAY RECAP *********************************************************************", "node2                      : ok=2    changed=1    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   ", "", "node2", "node1", "Using /etc/ansible/ansible.cfg as config file", "", "PLAY [Run remote profiler] *****************************************************", "", "TASK [Gathering Facts] *********************************************************", "ok: [node1]", "", "TASK [Run remote profiler] *****************************************************", "changed: [node1] => {\"ansible_job_id\": \"780942345903.22141\", \"changed\": true, \"finished\": 0, \"results_file\": \"/root/.ansible_async/780942345903.22141\", \"started\": 1}", "", "TASK [Check remote profiler] ***************************************************", "fatal: [node1]: FAILED! => {\"ansible_job_id\": \"780942345903.22141\", \"changed\": true, \"cmd\": [\"python3\", \"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\", \"-i\", \"0\", \"-t\", \"21154\"], \"delta\": \"0:00:00.121923\", \"end\": \"2023-09-18 15:06:33.021734\", \"finished\": 1, \"msg\": \"non-zero return code\", \"rc\": 1, \"start\": \"2023-09-18 15:06:32.899811\", \"stderr\": \"Traceback (most recent call last):\\n  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 563, in <module>\\n    main()\\n  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 554, in main\\n    real_main()\\n  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 551, in real_main\\n    parse_args()\\n  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 548, in parse_args\\n    server(args.port,args.perf_iteration,args.process_id)\\n  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 433, in server\\n    server = SimpleXMLRPCServer((hostname, port), allow_none=True)\\n  File \\\"/usr/lib/python3.6/xmlrpc/server.py\\\", line 599, in __init__\\n    socketserver.TCPServer.__init__(self, addr, requestHandler, bind_and_activate)\\n  File \\\"/usr/lib/python3.6/socketserver.py\\\", line 456, in __init__\\n    self.server_bind()\\n  File \\\"/usr/lib/python3.6/socketserver.py\\\", line 470, in server_bind\\n    self.socket.bind(self.server_address)\\nOSError: [Errno 98] Address already in use\", \"stderr_lines\": [\"Traceback (most recent call last):\", \"  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 563, in <module>\", \"    main()\", \"  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 554, in main\", \"    real_main()\", \"  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 551, in real_main\", \"    parse_args()\", \"  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 548, in parse_args\", \"    server(args.port,args.perf_iteration,args.process_id)\", \"  File \\\"/users/cseas002/HDSearch-Multinode/profiler/profiler.py\\\", line 433, in server\", \"    server = SimpleXMLRPCServer((hostname, port), allow_none=True)\", \"  File \\\"/usr/lib/python3.6/xmlrpc/server.py\\\", line 599, in __init__\", \"    socketserver.TCPServer.__init__(self, addr, requestHandler, bind_and_activate)\", \"  File \\\"/usr/lib/python3.6/socketserver.py\\\", line 456, in __init__\", \"    self.server_bind()\", \"  File \\\"/usr/lib/python3.6/socketserver.py\\\", line 470, in server_bind\", \"    self.socket.bind(self.server_address)\", \"OSError: [Errno 98] Address already in use\"], \"stdout\": \"\", \"stdout_lines\": []}", "", "PLAY RECAP *********************************************************************", "node1                      : ok=2    changed=1    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   ", "", "", "node2"]}

PLAY [Kill remote profiler] ****************************************************

PLAY RECAP *********************************************************************
node0                      : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

Using /users/cseas002/HDSearch-Multinode/ansible.cfg as config file
[WARNING]: While constructing a mapping from /users/cseas002/HDSearch-
Multinode/ansible/profiler.yml, line 20, column 3, found a duplicate dict key
(hosts). Using last defined value only.
[WARNING]: While constructing a mapping from /users/cseas002/HDSearch-
Multinode/ansible/profiler.yml, line 20, column 3, found a duplicate dict key
(tags). Using last defined value only.
[WARNING]: While constructing a mapping from /users/cseas002/HDSearch-
Multinode/ansible/profiler.yml, line 20, column 3, found a duplicate dict key
(tasks). Using last defined value only.

PLAY [Set profiler] ************************************************************

PLAY [Run remote profiler] *****************************************************

PLAY [Kill remote profiler] ****************************************************

TASK [Gathering Facts] *********************************************************
ok: [node2]
ok: [node1]

TASK [Run remote socwatch] *****************************************************
changed: [node1] => {"ansible_job_id": "186053078513.22365", "changed": true, "finished": 0, "results_file": "/users/cseas002/.ansible_async/186053078513.22365", "started": 1}
changed: [node2] => {"ansible_job_id": "754965690506.12999", "changed": true, "finished": 0, "results_file": "/users/cseas002/.ansible_async/754965690506.12999", "started": 1}

PLAY RECAP *********************************************************************
node1                      : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
node2                      : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

Using /users/cseas002/HDSearch-Multinode/ansible.cfg as config file

PLAY [Initialize cluster manager] **********************************************

PLAY [Initialize workers] ******************************************************

PLAY [Check Status of run] *****************************************************

TASK [Gathering Facts] *********************************************************
ok: [node0]

TASK [Check Status] ************************************************************
changed: [node0] => {"changed": true, "cmd": "~/HDSearch-Multinode/scripts/check-run-status.sh", "delta": "0:01:44.673780", "end": "2023-09-18 15:08:25.222774", "rc": 0, "start": "2023-09-18 15:06:40.548994", "stderr": "", "stderr_lines": [], "stdout": "", "stdout_lines": []}

PLAY [Leave Swarm] *************************************************************

PLAY RECAP *********************************************************************
node0                      : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

INFO:root:sudo python3 profiler/profiler.py -n node2 stop
INFO:root:sudo python3 ~/HDSearch-Multinode/profiler/profiler.py -n node1 stop
INFO:root:python3: can't open file '~/HDSearch-Multinode/profiler/profiler.py': [Errno 2] No such file or directory
INFO:root:sudo python3 ~/HDSearch-Multinode/profiler/profiler.py -n node2 report -d ~/data/SIXTH_RUN/turbo=False-kernelconfig=baseline-baseline-hyperthreading=False-qps=500-0/hdsearch/bucket_node2
INFO:root:python3: can't open file '~/HDSearch-Multinode/profiler/profiler.py': [Errno 2] No such file or directory
INFO:root:sudo python3 ~/HDSearch-Multinode/profiler/profiler.py -n node1 report -d ~/data/SIXTH_RUN/turbo=False-kernelconfig=baseline-baseline-hyperthreading=False-qps=500-0/hdsearch/midtier_node1
INFO:root:python3: can't open file '~/HDSearch-Multinode/profiler/profiler.py': [Errno 2] No such file or directory
INFO:root:./scripts/check-socwatch-status.sh node1
INFO:root:4
INFO:root:./scripts/check-socwatch-status.sh node2
INFO:root:4
INFO:root:sudo docker service logs microsuite_client --raw
INFO:root:client start
INFO:root:mv: cannot stat 'image_feature_vectors.dat': No such file or directory
INFO:root:rm -f *.o *.pb.cc *.pb.h load_generator_open_loop load_generator_closed_loop kill_index_server_empty
INFO:root:g++ -std=c++11 -O3 -mavx2 -mavx -fopenmp  -DMKL_ILP64 -m64 -I/opt/intel/mkl/include -I../ -I/usr/local/include -pthread -O3 -I../ -g -Wall -fopenmp -I../  -c -o helper_files/mid_tier_client_helper.o helper_files/mid_tier_client_helper.cc
INFO:root:[01m[Khelper_files/mid_tier_client_helper.cc:[m[K In function '[01m[Kvoid CreateDatasetFromBinaryFile(const string&, MultiplePoints*)[m[K':
INFO:root:[01m[Khelper_files/mid_tier_client_helper.cc:296:77:[m[K [01;35m[Kwarning: [m[Kcomparison between signed and unsigned integer expressions [[01;35m[K-Wsign-compare[m[K]
INFO:root:         if([01;35m[Kfread(values, sizeof(float), dataset_dimensions, dataset_binary) == dataset_dimensions[m[K)
INFO:root:            [01;35m[K~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~[m[K
INFO:root:[01m[Khelper_files/mid_tier_client_helper.cc:[m[K In function '[01m[Kvoid ResetMetaStats(GlobalStats*, int)[m[K':
INFO:root:[01m[Khelper_files/mid_tier_client_helper.cc:586:31:[m[K [01;35m[Kwarning: [m[Kcomparison between signed and unsigned integer expressions [[01;35m[K-Wsign-compare[m[K]
INFO:root:     for(unsigned int i = 0; [01;35m[Ki < number_of_bucket_servers[m[K; i++)
INFO:root:                             [01;35m[K~~^~~~~~~~~~~~~~~~~~~~~~~~~~[m[K
INFO:root:g++ -std=c++11 -O3 -mavx2 -mavx -fopenmp  -DMKL_ILP64 -m64 -I/opt/intel/mkl/include -I../ -I/usr/local/include -pthread -O3 -I../ -g -Wall -fopenmp -I../  -c -o load_generator_open_loop.o load_generator_open_loop.cc
INFO:root:[01m[Kload_generator_open_loop.cc:[m[K In function '[01m[Kint main(int, char**)[m[K':
INFO:root:[01m[Kload_generator_open_loop.cc:304:51:[m[K [01;35m[Kwarning: [m[Kcomparison between signed and unsigned integer expressions [[01;35m[K-Wsign-compare[m[K]
INFO:root:     while ([01;35m[Kresponses_recvd->AtomicallyReadCount() < overall_queries[m[K)
INFO:root:            [01;35m[K~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~[m[K
INFO:root:[01m[Kload_generator_open_loop.cc:306:75:[m[K [01;35m[Kwarning: [m[Kcomparison between signed and unsigned integer expressions [[01;35m[K-Wsign-compare[m[K]
INFO:root:         if (curr_time >= next_time && [01;35m[Knum_requests->AtomicallyReadCount() < overall_queries[m[K)
INFO:root:                                       [01;35m[K~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~[m[K
INFO:root:g++ ../protoc_files/mid_tier.pb.o ../protoc_files/mid_tier.grpc.pb.o ../bucket_service/src/multiple_points.o ../bucket_service/src/point.o ../bucket_service/src/utils.o ../bucket_service/src/dist_calc.o ../bucket_service/src/custom_priority_queue.o helper_files/mid_tier_client_helper.o helper_files/timing.o helper_files/utils.o load_generator_open_loop.o -O3 -o load_generator_open_loop -L/usr/local/lib `pkg-config --libs grpc++ grpc` -lprotobuf -lpthread -I /usr/local/include -lflann -fopenmp -L/usr/lib64 -lstdc++ -lssl -lcrypto -fopenmp -Wl,--start-group /opt/intel/mkl/lib/intel64/libmkl_intel_ilp64.a /opt/intel/mkl/lib/intel64/libmkl_gnu_thread.a /opt/intel/mkl/lib/intel64/libmkl_core.a -Wl,--end-group -lgomp -lpthread -lm -ldl -I../
INFO:root:g++ -std=c++11 -O3 -mavx2 -mavx -fopenmp  -DMKL_ILP64 -m64 -I/opt/intel/mkl/include -I../ -I/usr/local/include -pthread -O3 -I../ -g -Wall -fopenmp -I../  -c -o load_generator_closed_loop.o load_generator_closed_loop.cc
INFO:root:g++ ../protoc_files/mid_tier.pb.o ../protoc_files/mid_tier.grpc.pb.o ../bucket_service/src/multiple_points.o ../bucket_service/src/point.o ../bucket_service/src/utils.o ../bucket_service/src/dist_calc.o ../bucket_service/src/custom_priority_queue.o helper_files/mid_tier_client_helper.o helper_files/timing.o helper_files/utils.o load_generator_closed_loop.o -O3 -o load_generator_closed_loop -L/usr/local/lib `pkg-config --libs grpc++ grpc` -lprotobuf -lpthread -I /usr/local/include -lflann -fopenmp -L/usr/lib64 -lstdc++ -lssl -lcrypto -fopenmp -Wl,--start-group /opt/intel/mkl/lib/intel64/libmkl_intel_ilp64.a /opt/intel/mkl/lib/intel64/libmkl_gnu_thread.a /opt/intel/mkl/lib/intel64/libmkl_core.a -Wl,--end-group -lgomp -lpthread -lm -ldl -I../
INFO:root:g++ -std=c++11 -O3 -mavx2 -mavx -fopenmp  -DMKL_ILP64 -m64 -I/opt/intel/mkl/include -I../ -I/usr/local/include -pthread -O3 -I../ -g -Wall -fopenmp -I../  -c -o kill_index_server_empty.o kill_index_server_empty.cc
INFO:root:g++ ../protoc_files/mid_tier.pb.o ../protoc_files/mid_tier.grpc.pb.o ../bucket_service/src/multiple_points.o ../bucket_service/src/point.o ../bucket_service/src/utils.o ../bucket_service/src/dist_calc.o ../bucket_service/src/custom_priority_queue.o helper_files/mid_tier_client_helper.o helper_files/timing.o helper_files/utils.o kill_index_server_empty.o -O3 -o kill_index_server_empty -L/usr/local/lib `pkg-config --libs grpc++ grpc` -lprotobuf -lpthread -I /usr/local/include -lflann -fopenmp -L/usr/lib64 -lstdc++ -lssl -lcrypto -fopenmp -Wl,--start-group /opt/intel/mkl/lib/intel64/libmkl_intel_ilp64.a /opt/intel/mkl/lib/intel64/libmkl_gnu_thread.a /opt/intel/mkl/lib/intel64/libmkl_core.a -Wl,--end-group -lgomp -lpthread -lm -ldl -I../
INFO:root:         @    @       �           ?                midtier launched
INFO:root:profilers launched
INFO:root:mkdir: cannot create directory './results': File exists
INFO:root:1 4 32 # Requests: 0 
INFO:root:# Responses: 0 
INFO:root:!!!!!!!End of Warmup Period!!!!!!! 
INFO:root:1 4 32 End of Actual Run
INFO:root:# Requests: 59915 
INFO:root:# Responses: 59914 
INFO:root:Achieved QPS: 499.283 
INFO:root:Average Response Time(ms): 1.03371 
INFO:root:0.842 0.903 0.949 0.987 1.023 1.06 1.097 1.139 1.198 1.248 1.376 4.089 0.842 0.903 0.949 0.987 1.023 1.06 1.097 1.139 1.198 1.248 1.376 4.089 
INFO:root: Total response time 
INFO:root:0.842 0.903 0.949 0.987 1.023 1.06 1.097 1.139 1.198 1.248 1.376 4.089 
INFO:root:
INFO:root: Index creation time 0 0 0 0 0 0 0 0 0 0 0 0 
INFO:root: Update index util time 0 0 0 0 0 0 0 0 0 0 0 0 
INFO:root: Unpack loadgen request time 0.022 0.032 0.04 0.051 0.053 0.057 0.068 0.075 0.08 0.084 0.092 0.102 
INFO:root: Get point ids time 
INFO:root:0.05 0.069 0.088 0.109 0.123 0.14 0.162 0.202 0.27 0.309 0.356 0.409 
INFO:root: Total time taken by index server: 
INFO:root:0.239 0.272 0.302 0.33 0.363 0.4 0.438 0.474 0.522 0.557 0.613 1.368 Average Index Time(ms): 0.376132 
INFO:root:
INFO:root: Get bucket responses time 
INFO:root:0.394 0.421 0.443 0.464 0.484 0.506 0.53 0.56 0.602 0.639 0.724 2.895 Average Bucket Time(ms): 0.498374 
INFO:root:
INFO:root: Create bucket request time 0 0 0 0 0 0 0 0 0 0 0 0 
INFO:root: Unpack bucket request time 0.004 0.005 0.006 0.007 0.009 0.017 0.021 0.022 0.024 0.026 0.036 0.048 
INFO:root: Calculate knn time 
INFO:root:0.036 0.038 0.04 0.043 0.046 0.054 0.084 0.11 0.143 0.157 0.171 0.189 
INFO:root: Pack bucket response time 0 0 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.002 0.002 0.016 
INFO:root: Unpack bucket response time 0 0 0 0 0 0.001 0.001 0.001 0.001 0.001 0.002 0.015 
INFO:root: Merge time 0.011 0.012 0.014 0.017 0.019 0.025 0.028 0.038 0.048 0.062 0.085 0.113 
INFO:root: Pack index response time 0.011 0.012 0.014 0.017 0.019 0.025 0.028 0.038 0.048 0.062 0.085 0.113 
INFO:root: Unpack index response time 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.002 0.002 0.002 0.002 0.003 
INFO:root: 4.00574e-05
INFO:root:got kill ack
INFO:root:[libprotobuf FATAL /usr/local/include/google/protobuf/repeated_field.h:1535] CHECK failed: (index) < (current_size_): 
INFO:root:terminate called after throwing an instance of 'google::protobuf::FatalException'
INFO:root:  what():  CHECK failed: (index) < (current_size_): 
INFO:root:sudo touch ~/data/SIXTH_RUN/turbo=False-kernelconfig=baseline-baseline-hyperthreading=False-qps=500-0/hdsearch_client
INFO:root:touch: cannot touch '~/data/SIXTH_RUN/turbo=False-kernelconfig=baseline-baseline-hyperthreading=False-qps=500-0/hdsearch_client': No such file or directory
INFO:root:sudo chmod 777 ~/data/SIXTH_RUN/turbo=False-kernelconfig=baseline-baseline-hyperthreading=False-qps=500-0/hdsearch_client
INFO:root:chmod: cannot access '~/data/SIXTH_RUN/turbo=False-kernelconfig=baseline-baseline-hyperthreading=False-qps=500-0/hdsearch_client': No such file or directory
ansible-playbook -v -i hosts -e "" --tags "setup_docker" ansible/install.yml
ansible-playbook -v -i hosts -e "" --tags "set_profiler_hosts" ansible/profiler.yml
ansible-playbook -v -i hosts -e "" --tags "leave_swarm" ansible/hdsearch.yml
ansible-playbook -v -i hosts -e "" --tags "init_master" ansible/hdsearch.yml
ansible-playbook -v -i hosts -e "" --tags "init_workers" ansible/hdsearch.yml
ansible-playbook -v -i hosts -e "MSR_VALUE=0x1414"  ansible/configure.yml
ansible-playbook -v -i hosts -e "" --tags "kill_profiler" ansible/profiler.yml
ansible-playbook -v -i hosts -e "HOST_FILE=~/HDSearch-Multinode/hosts ITERATION=0" --tags "run_profiler" ansible/profiler.yml
Profilerrrrr putput 0
ansible-playbook -v -i hosts -e "MONITOR_TIME=40 OUTPUT_FILE=turbo=False-kernelconfig=baseline-baseline-hyperthreading=False-qps=500" --tags "run_socwatch" ./ansible/profiler.yml
ansible-playbook -v -i hosts -e "" --tags "check_status" ansible/hdsearch.yml
Traceback (most recent call last):
  File "run_experiment.py", line 435, in <module>
    main(sys.argv[1:])
  File "run_experiment.py", line 431, in main
    run_multiple_experiments('~/data', batch_name, system_conf, client_conf, midtier_conf, bucket_conf, iter)
  File "run_experiment.py", line 355, in run_multiple_experiments
    status = run_single_experiment(system_conf,root_results_dir, name_prefix, instance_conf, iter,bucket,midtier)
  File "run_experiment.py", line 317, in run_single_experiment
    with open(client_results_path_name, 'w') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '~/data/SIXTH_RUN/turbo=False-kernelconfig=baseline-baseline-hyperthreading=False-qps=500-0/hdsearch_client'
